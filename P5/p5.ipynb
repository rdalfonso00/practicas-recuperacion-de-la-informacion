{"cells":[{"cell_type":"markdown","metadata":{"id":"F4qwv6V8DiGv"},"source":["# Laboratorio 5\n","\n","1. Utilice los archivos pre-procesados que obtuvo en la práctica 3, mismos que almacenan la colección CACM y los queries.\n","2. Si en la práctica tres generó el vocabulario sin ordenarlo alfabéticamente ordénelo, tanto el normal como el reducido.\n","3. Represente cada documento empleando el modelo vectorial y el esquema de pesado tf. Recuerde que la dimensión de los vectores será igual al tamaño del vocabulario que obtuvo en la práctica 4. Trabajen con el vocabulario sin reducir y después reducido. Guarda las matrices generada, que representa los documentos de la colección, para utilizarla en prácticas posteriores.\n","4. Utiliza el archivo de queries y represéntelos también empleando el modelo vectorial y esquema de pesado tf. Con vocabulario normal y reducido. Guarda las matrices generadas."]},{"cell_type":"markdown","metadata":{},"source":["Primero extraemos las palabras de documentos_final.txt\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['preliminari', 'report', 'intern', 'algebra', 'languag', 'extract', 'root', 'repeat', 'subtract', 'digit']\n","3204\n"]}],"source":["filename = \"documentos_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","doc_words = \"\"\n","doc_count = 0\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        #doc_data[i]['d_num'] = split[0]\n","        doc_words += split[2] + \" \"\n","    elif len(split) == 3:\n","        doc_words += split[1] + \" \"\n","    doc_count += 1\n","doc_words = [x for x in (doc_words).split(\" \") if x]\n","print(doc_words[:10])\n","print(doc_count)"]},{"cell_type":"markdown","metadata":{},"source":["Luego, importamos el vocabulario creado en la práctica 4."]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["filename = \"vocabulario_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","vocabulary = file.read().split(\", \")\n","file.close()\n","vocabulary = sorted(vocabulary)"]},{"cell_type":"markdown","metadata":{},"source":["Ahora, comenzamos los cálculos para la representación del vocabulario por medio del modelo vectorial."]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["{'1604a': 1,\n"," '16th': 1,\n"," '1970': 1,\n"," '2': 1,\n"," '263a': 1,\n"," '2f': 1,\n"," '2k': 2,\n"," '2m': 1,\n"," '2n': 11,\n"," '32k': 2,\n"," '3n': 1,\n"," '40k': 1,\n"," '44x': 1,\n"," '473l': 2,\n"," '4k': 1,\n"," '4n': 3,\n"," '4x16': 1,\n"," '5n': 1,\n"," '64k': 1,\n"," '7th': 1,\n"," '8k': 1,\n"," '8n': 1,\n"," '9n': 1,\n"," 'a': 4,\n"," 'a1': 28,\n"," 'a2': 1,\n"," 'a418': 1,\n"," 'a419': 1,\n"," 'a420': 1,\n"," 'a421': 1,\n"," 'a422': 1,\n"," 'a423': 1,\n"," 'a424': 1,\n"," 'a425': 1,\n"," 'a426': 1,\n"," 'a427': 1,\n"," 'a428': 1,\n"," 'a429': 1,\n"," 'a430': 1,\n"," 'a432': 1,\n"," 'a433': 1,\n"," 'a434': 1,\n"," 'a435': 1,\n"," 'a436': 1,\n"," 'a437': 1,\n"," 'a438': 1,\n"," 'a439': 1,\n"," 'a440': 1,\n"," 'a441': 1,\n"," 'a442': 1,\n"," 'a443': 1,\n"," 'a444': 1,\n"," 'a445': 1,\n"," 'a446': 1,\n"," 'a448': 1,\n"," 'a449': 1,\n"," 'a450': 1,\n"," 'a451': 1,\n"," 'a452': 1,\n"," 'a453': 1,\n"," 'a454': 1,\n"," 'a455': 1,\n"," 'a456': 1,\n"," 'a457': 1,\n"," 'a458': 1,\n"," 'a459': 1,\n"," 'a460': 1,\n"," 'a461': 1,\n"," 'a462': 1,\n"," 'a463': 1,\n"," 'a464': 1,\n"," 'a465': 1,\n"," 'a466': 1,\n"," 'a467': 1,\n"," 'a468': 1,\n"," 'a469': 1,\n"," 'a470': 1,\n"," 'a471': 1,\n"," 'a472': 1,\n"," 'a473': 1,\n"," 'a474': 1,\n"," 'a475': 1,\n"," 'a476': 1,\n"," 'a477': 1,\n"," 'a478': 1,\n"," 'a479': 1,\n"," 'a480': 1,\n"," 'a481': 1,\n"," 'a482': 1,\n"," 'a483': 1,\n"," 'a484': 1,\n"," 'a485': 1,\n"," 'a486': 1,\n"," 'a487': 1,\n"," 'aaron': 1,\n"," 'abac': 1,\n"," 'abacu': 1,\n"," 'abandon': 2,\n"," 'abbott': 1,\n"," 'abbrevi': 5,\n"," 'abcd': 1,\n"," 'abd': 1,\n"," 'abil': 30,\n"," 'abl': 16,\n"," 'abscissa': 3,\n"," 'absenc': 7,\n"," 'absolut': 11,\n"," 'absorb': 1,\n"," 'absorpt': 1,\n"," 'abstract': 62,\n"," 'academ': 11,\n"," 'academician': 1,\n"," 'accel': 6,\n"," 'accent': 1,\n"," 'accept': 39,\n"," 'access': 119,\n"," 'accid': 1,\n"," 'accommod': 4,\n"," 'accompani': 3,\n"," 'accomplish': 31,\n"," 'accord': 26,\n"," 'account': 38,\n"," 'accru': 2,\n"," 'accumul': 25,\n"," 'accur': 30,\n"," 'accuraci': 45,\n"," 'acf': 1,\n"," 'achiev': 75,\n"," 'acknowledg': 1,\n"," 'acl': 1,\n"," 'acm': 41,\n"," 'aco': 1,\n"," 'acqui': 1,\n"," 'acquir': 6,\n"," 'acquisit': 2,\n"," 'acronym': 1,\n"," 'act': 3,\n"," 'action': 37,\n"," 'activ': 79,\n"," 'actor': 1,\n"," 'actual': 46,\n"," 'acycl': 3,\n"," 'ad': 42,\n"," 'ada': 1,\n"," 'adam': 3,\n"," 'adapt': 41,\n"," 'add': 9,\n"," 'addend': 3,\n"," 'addendum': 5,\n"," 'adder': 4,\n"," 'addit': 124,\n"," 'address': 86,\n"," 'adept': 2,\n"," 'adequ': 13,\n"," 'adequaci': 5,\n"," 'adher': 1,\n"," 'adjac': 9,\n"," 'adjoin': 1,\n"," 'adjoint': 1,\n"," 'adjudg': 1,\n"," 'adjust': 14,\n"," 'administr': 21,\n"," 'admir': 1,\n"," 'admiss': 6,\n"," 'admit': 7,\n"," 'adopt': 10,\n"," 'adp': 1,\n"," 'advanc': 27,\n"," 'advantag': 89,\n"," 'advent': 3,\n"," 'adver': 2,\n"," 'advi': 1,\n"," 'advic': 1,\n"," 'advoc': 3,\n"," 'aeronaut': 2,\n"," 'aesthet': 1,\n"," 'affair': 1,\n"," 'affect': 18,\n"," 'affirm': 2,\n"," 'affix': 6,\n"," 'afford': 2,\n"," 'age': 3,\n"," 'agement': 1,\n"," 'agenc': 7,\n"," 'agenda': 1,\n"," 'agent': 1,\n"," 'aggrav': 1,\n"," 'aggreg': 12,\n"," 'ago': 4,\n"," 'agr': 3,\n"," 'agreement': 3,\n"," 'ahead': 1,\n"," 'ahren': 5,\n"," 'ai': 2,\n"," 'aid': 61,\n"," 'aim': 16,\n"," 'air': 4,\n"," 'aircraft': 2,\n"," 'airi': 2,\n"," 'airlin': 2,\n"," 'aitken': 2,\n"," 'akin': 1,\n"," 'al': 5,\n"," 'alan': 1,\n"," 'alarm': 1,\n"," 'albani': 1,\n"," 'albeit': 1,\n"," 'alcor': 1,\n"," 'aldep': 1,\n"," 'alert': 2,\n"," 'alg': 2,\n"," 'algebra': 78,\n"," 'algem': 3,\n"," 'algol': 143,\n"," 'algorithm': 1739,\n"," 'alia': 1,\n"," 'alien': 1,\n"," 'align': 5,\n"," 'alik': 1,\n"," 'alleg': 1,\n"," 'allegedli': 1,\n"," 'allevi': 3,\n"," 'alloc': 123,\n"," 'allow': 73,\n"," 'aloc': 1,\n"," 'aloof': 1,\n"," 'alpak': 1,\n"," 'alpha': 3,\n"," 'alphabet': 15,\n"," 'alphanum': 7,\n"," 'alphard': 2,\n"," 'alter': 13,\n"," 'altern': 69,\n"," 'althm': 1,\n"," 'altogeth': 1,\n"," 'altran': 1,\n"," 'ambigu': 27,\n"," 'ambit': 4,\n"," 'ambiti': 2,\n"," 'amen': 1,\n"," 'amend': 1,\n"," 'american': 12,\n"," 'amesplot': 3,\n"," 'ammunit': 1,\n"," 'amount': 57,\n"," 'amp': 1,\n"," 'amphisbaen': 1,\n"," 'amplifi': 1,\n"," 'amplitud': 2,\n"," 'an': 2,\n"," 'analog': 18,\n"," 'analogu': 2,\n"," 'analysi': 253,\n"," 'analyst': 5,\n"," 'analyt': 41,\n"," 'analyz': 111,\n"," 'anarc': 1,\n"," 'anatomi': 1,\n"," 'ancestor': 3,\n"," 'ancient': 1,\n"," 'andri': 1,\n"," 'anew': 1,\n"," 'angel': 1,\n"," 'angl': 12,\n"," 'angular': 1,\n"," 'anim': 9,\n"," 'annex': 1,\n"," 'anniversari': 1,\n"," 'annot': 1,\n"," 'announc': 3,\n"," 'annoy': 1,\n"," 'annual': 2,\n"," 'anom': 4,\n"," 'anomali': 8,\n"," 'anpak': 1,\n"," 'ansi': 1,\n"," 'answer': 41,\n"," 'anticip': 5,\n"," 'anticipatori': 2,\n"," 'antilog': 1,\n"," 'aparel': 2,\n"," 'apertur': 5,\n"," 'apl': 5,\n"," 'apld': 2,\n"," 'appar': 5,\n"," 'appeal': 1,\n"," 'appear': 34,\n"," 'append': 3,\n"," 'appendix': 11,\n"," 'appli': 124,\n"," 'applic': 297,\n"," 'appoint': 4,\n"," 'apport': 1,\n"," 'apprai': 2,\n"," 'appreci': 3,\n"," 'approach': 139,\n"," 'appropri': 6,\n"," 'approxim': 138,\n"," 'apr': 2,\n"," 'april': 2,\n"," 'apt': 3,\n"," 'aquat': 1,\n"," 'ar': 1,\n"," 'arbitr': 1,\n"," 'arbitrari': 51,\n"," 'arbitrarili': 6,\n"," 'arboresc': 1,\n"," 'arc': 8,\n"," 'arccosin': 1,\n"," 'arccossin': 1,\n"," 'archaeolog': 1,\n"," 'archimedian': 1,\n"," 'architectur': 22,\n"," 'arctang': 2,\n"," 'area': 99,\n"," 'areext': 1,\n"," 'argentina': 2,\n"," 'argu': 14,\n"," 'argument': 26,\n"," 'ari': 32,\n"," 'arithmet': 97,\n"," 'arm': 3,\n"," 'armament': 1,\n"," 'armi': 2,\n"," 'aro': 1,\n"," 'arpa': 5,\n"," 'arpanet': 2,\n"," 'arrang': 22,\n"," 'array': 79,\n"," 'arrhythmia': 1,\n"," 'arriv': 16,\n"," 'arrow': 1,\n"," 'art': 9,\n"," 'arteri': 2,\n"," 'arthur': 1,\n"," 'articl': 21,\n"," 'articul': 1,\n"," 'artifici': 14,\n"," 'artist': 1,\n"," 'asa': 6,\n"," 'ascend': 4,\n"," 'ascertain': 5,\n"," 'ascii': 5,\n"," 'ashenhurst': 2,\n"," 'ask': 5,\n"," 'asp': 4,\n"," 'aspect': 56,\n"," 'ass': 3,\n"," 'assembl': 52,\n"," 'assert': 23,\n"," 'assess': 10,\n"," 'assign': 63,\n"," 'assimil': 1,\n"," 'assist': 29,\n"," 'assoc': 1,\n"," 'associ': 32,\n"," 'assort': 2,\n"," 'assum': 48,\n"," 'assumpt': 31,\n"," 'assur': 5,\n"," 'asterionella': 1,\n"," 'astronaut': 1,\n"," 'astronomi': 1,\n"," 'asymmetr': 2,\n"," 'asymptot': 15,\n"," 'asynchron': 7,\n"," 'ativ': 2,\n"," 'atla': 7,\n"," 'atmosph': 1,\n"," 'atom': 7,\n"," 'attach': 7,\n"," 'attack': 4,\n"," 'attain': 8,\n"," 'attempt': 52,\n"," 'attend': 2,\n"," 'attent': 19,\n"," 'attenu': 1,\n"," 'attitud': 10,\n"," 'attract': 9,\n"," 'attribut': 23,\n"," 'auction': 2,\n"," 'audio': 2,\n"," 'augment': 13,\n"," 'august': 2,\n"," 'australia': 1,\n"," 'authent': 9,\n"," 'author': 46,\n"," 'auto': 2,\n"," 'autocod': 3,\n"," 'autocorrel': 5,\n"," 'autocovari': 1,\n"," 'autom': 34,\n"," 'automat': 115,\n"," 'automaton': 13,\n"," 'automobil': 1,\n"," 'automorph': 2,\n"," 'autonom': 2,\n"," 'auxiliari': 16,\n"," 'avail': 11,\n"," 'avenu': 4,\n"," 'aver': 1,\n"," 'averag': 64,\n"," 'avint': 1,\n"," 'avl': 11,\n"," 'avoid': 39,\n"," 'await': 1,\n"," 'awaken': 1,\n"," 'awar': 3,\n"," 'ax': 6,\n"," 'axi': 5,\n"," 'axial': 1,\n"," 'axiom': 13,\n"," 'axiomat': 3,\n"," 'axl': 1,\n"," 'ay1': 1,\n"," 'b1': 5,\n"," 'b2': 2,\n"," 'b205': 1,\n"," 'b3': 3,\n"," 'b5500': 4,\n"," 'b6500': 2,\n"," 'babylonian': 1,\n"," 'bachelor': 1,\n"," 'back': 17,\n"," 'backboard': 1,\n"," 'background': 17,\n"," 'backtrack': 6,\n"," 'backup': 3,\n"," 'backward': 10,\n"," 'bad': 5,\n"," 'bag': 1,\n"," 'bagdam': 2,\n"," 'bairstow': 7,\n"," 'baker': 1,\n"," 'balanc': 23,\n"," 'ballist': 1,\n"," 'baltimor': 1,\n"," 'band': 6,\n"," 'bandmatrix': 1,\n"," 'bandsolv': 2,\n"," 'bandwidth': 2,\n"," 'bank': 28,\n"," 'banker': 1,\n"," 'bar': 1,\n"," 'baran': 1,\n"," 'bargraph': 1,\n"," 'barnett': 1,\n"," 'barrier': 4,\n"," 'barstow': 1,\n"," 'base': 259,\n"," 'bashforth': 1,\n"," 'basi': 60,\n"," 'basic': 119,\n"," 'basin': 1,\n"," 'basser': 1,\n"," 'batch': 31,\n"," 'bauer': 1,\n"," 'bay': 2,\n"," 'bb': 1,\n"," 'bbn': 2,\n"," 'bc': 1,\n"," 'bcd': 6,\n"," 'bdl': 2,\n"," 'be': 1,\n"," 'beal': 3,\n"," 'bear': 4,\n"," 'beat': 1,\n"," 'bedford': 1,\n"," 'begin': 17,\n"," 'beginn': 2,\n"," 'begun': 2,\n"," 'behav': 1,\n"," 'behavior': 49,\n"," 'bei': 3,\n"," 'bel': 1,\n"," 'beladi': 1,\n"," 'belfap': 1,\n"," 'belief': 4,\n"," 'believ': 4,\n"," 'bell': 7,\n"," 'bellman': 5,\n"," 'belong': 5,\n"," 'belzer': 1,\n"," 'bench': 1,\n"," 'benchmark': 2,\n"," 'bend': 1,\n"," 'bendix': 1,\n"," 'benefici': 2,\n"," 'benefit': 18,\n"," 'ber': 3,\n"," 'berkeley': 3,\n"," 'bernard': 2,\n"," 'bernstein': 2,\n"," 'beset': 2,\n"," 'bessel': 17,\n"," 'bestow': 1,\n"," 'beta': 7,\n"," 'bezout': 3,\n"," 'bia': 4,\n"," 'bibliograph': 3,\n"," 'bibliographi': 7,\n"," 'biconnect': 1,\n"," 'bicub': 1,\n"," 'bidirect': 2,\n"," 'big': 1,\n"," 'bigger': 3,\n"," 'biharmon': 2,\n"," 'bill': 3,\n"," 'binari': 94,\n"," 'bind': 8,\n"," 'binomi': 6,\n"," 'biochem': 4,\n"," 'biochemci': 1,\n"," 'biolog': 9,\n"," 'biom': 1,\n"," 'bipartit': 3,\n"," 'bipha': 1,\n"," 'bisect': 3,\n"," 'bit': 90,\n"," 'bitwi': 1,\n"," 'bivari': 5,\n"," 'bk': 1,\n"," 'black': 3,\n"," 'blank': 3,\n"," 'blend': 1,\n"," 'blind': 7,\n"," 'bliss': 1,\n"," 'blnsi': 1,\n"," 'block': 103,\n"," 'blood': 2,\n"," 'blown': 1,\n"," 'bmd': 2,\n"," 'bmdp': 1,\n"," 'bnf': 5,\n"," 'board': 7,\n"," 'bobrow': 1,\n"," 'bodi': 11,\n"," 'bohm': 1,\n"," 'bonanza': 1,\n"," 'bond': 2,\n"," 'bonu': 2,\n"," 'boo': 1,\n"," 'book': 3,\n"," 'booker': 1,\n"," 'bookkeep': 8,\n"," 'boolean': 34,\n"," 'boot': 1,\n"," 'boothroyd': 1,\n"," 'bootstrap': 6,\n"," 'border': 1,\n"," 'borrow': 1,\n"," 'boston': 1,\n"," 'bottleneck': 2,\n"," 'bottom': 7,\n"," 'bound': 86,\n"," 'boundari': 40,\n"," 'box': 3,\n"," 'boyer': 1,\n"," 'bracket': 1,\n"," 'brad': 2,\n"," 'braill': 6,\n"," 'brailler': 2,\n"," 'brain': 3,\n"," 'branch': 17,\n"," 'breach': 3,\n"," 'breadboard': 1,\n"," 'break': 5,\n"," 'breakdown': 3,\n"," 'breakthrough': 1,\n"," 'brent': 1,\n"," 'briefli': 35,\n"," 'brinch': 2,\n"," 'bring': 8,\n"," 'bro': 1,\n"," 'broad': 10,\n"," 'broadcast': 6,\n"," 'broaden': 1,\n"," 'broader': 3,\n"," 'broadli': 1,\n"," 'broken': 4,\n"," 'bromwich': 1,\n"," 'brooker': 1,\n"," 'brookhaven': 2,\n"," 'brother': 3,\n"," 'brought': 3,\n"," 'brow': 1,\n"," 'brown': 2,\n"," 'brute': 1,\n"," 'bryant': 2,\n"," 'bu': 1,\n"," 'bubbl': 3,\n"," 'bubget': 1,\n"," 'bucket': 9,\n"," 'buddi': 18,\n"," 'budget': 6,\n"," 'buffalo': 1,\n"," 'buffer': 32,\n"," 'buford': 1,\n"," 'bug': 1,\n"," 'bugsi': 2,\n"," 'build': 23,\n"," 'buildabl': 1,\n"," 'built': 20,\n"," 'bulirsh': 1,\n"," 'bulk': 3,\n"," 'bulki': 1,\n"," 'bundl': 1,\n"," 'burden': 2,\n"," 'burdensom': 1,\n"," 'bureau': 2,\n"," 'burnett': 1,\n"," 'burrough': 7,\n"," 'burst': 1,\n"," 'burstal': 1,\n"," 'busi': 32,\n"," 'butcher': 2,\n"," 'button': 1,\n"," 'buy': 2,\n"," 'bx': 2,\n"," 'byan': 1,\n"," 'bypass': 5,\n"," 'byproduct': 1,\n"," 'byte': 8,\n"," 'c1': 6,\n"," 'c191': 1,\n"," 'c2': 9,\n"," 'c266': 1,\n"," 'c3': 1,\n"," 'c363': 1,\n"," 'c379': 1,\n"," 'c380': 1,\n"," 'c386': 1,\n"," 'c404': 1,\n"," 'c407': 1,\n"," 'c446': 1,\n"," 'c451': 1,\n"," 'c5': 9,\n"," 'c6': 12,\n"," 'cabl': 1,\n"," 'cach': 5,\n"," 'cacm': 1,\n"," 'cadep': 2,\n"," 'cai': 2,\n"," 'calcul': 110,\n"," 'calculu': 19,\n"," 'calendar': 6,\n"," 'calibr': 1,\n"," 'california': 7,\n"," 'call': 119,\n"," 'caller': 1,\n"," 'callup': 1,\n"," 'calorimet': 1,\n"," 'cam': 2,\n"," 'cambridg': 1,\n"," 'came': 2,\n"," 'camera': 1,\n"," 'campaign': 1,\n"," 'campu': 2,\n"," 'cancel': 1,\n"," 'candid': 4,\n"," 'canon': 5,\n"," 'cap': 5,\n"," 'capabl': 92,\n"," 'capac': 16,\n"," 'capacit': 2,\n"," 'capit': 1,\n"," 'captur': 1,\n"," 'car': 7,\n"," 'card': 15,\n"," 'cardiac': 1,\n"," 'cardin': 2,\n"," 'cardiographi': 1,\n"," 'cardiovascular': 1,\n"," 'care': 19,\n"," 'carlo': 9,\n"," 'carnegi': 2,\n"," 'carolina': 2,\n"," 'carotid': 2,\n"," 'carri': 26,\n"," 'cartesian': 1,\n"," 'cartoon': 1,\n"," 'cascad': 3,\n"," 'case': 150,\n"," 'casework': 2,\n"," 'cash': 1,\n"," 'cast': 1,\n"," 'casual': 3,\n"," 'catalog': 6,\n"," 'catalogu': 2,\n"," 'catalyst': 1,\n"," 'catalystincorpor': 1,\n"," 'catastroph': 1,\n"," 'catch': 1,\n"," 'categor': 2,\n"," 'categori': 7,\n"," 'cater': 1,\n"," 'cathod': 6,\n"," 'catmul': 1,\n"," 'cau': 13,\n"," 'cauchi': 2,\n"," 'caught': 1,\n"," 'ccc': 2,\n"," 'ccd': 2,\n"," 'cdc': 19,\n"," 'cdr': 3,\n"," 'celesti': 4,\n"," 'cell': 25,\n"," 'cellar': 1,\n"," 'cellular': 7,\n"," 'censu': 3,\n"," 'cent': 1,\n"," 'center': 33,\n"," 'central': 31,\n"," 'centroid': 1,\n"," 'cep': 5,\n"," 'certainli': 2,\n"," 'certif': 9,\n"," 'cf': 2,\n"," 'cft': 3,\n"," 'ch': 1,\n"," 'chain': 30,\n"," 'chair': 1,\n"," 'chairman': 4,\n"," 'challeng': 2,\n"," 'chamber': 3,\n"," 'championship': 1,\n"," 'chanc': 2,\n"," 'chang': 46,\n"," 'changeabl': 1,\n"," 'channel': 13,\n"," 'charact': 137,\n"," 'characterist': 97,\n"," 'charg': 5,\n"," 'chargeout': 3,\n"," 'charl': 1,\n"," 'charlier': 2,\n"," 'chart': 10,\n"," 'chaudhuri': 1,\n"," 'cheapli': 1,\n"," 'chebyschev': 4,\n"," 'chebyshev': 24,\n"," 'check': 43,\n"," 'checker': 4,\n"," 'checkerboard': 1,\n"," 'checkout': 2,\n"," 'checkpoint': 4,\n"," 'checksum': 1,\n"," 'chemic': 6,\n"," 'chemistri': 3,\n"," 'chen': 1,\n"," 'cheney': 2,\n"," 'chess': 8,\n"," 'chi': 4,\n"," 'chief': 4,\n"," 'chile': 2,\n"," 'china': 2,\n"," 'chine': 7,\n"," 'chino': 2,\n"," 'chip': 1,\n"," 'choic': 43,\n"," 'chomski': 3,\n"," 'choo': 15,\n"," 'chop': 4,\n"," 'chosen': 24,\n"," 'chrisoffel': 1,\n"," 'christoffel': 1,\n"," 'chromosom': 1,\n"," 'chronolog': 3,\n"," 'chunk': 1,\n"," 'church': 3,\n"," 'cigarett': 1,\n"," 'cincinnati': 2,\n"," 'cipher': 11,\n"," 'cipi': 1,\n"," 'circa': 2,\n"," 'circl': 8,\n"," 'circuit': 29,\n"," 'circul': 2,\n"," 'circular': 14,\n"," 'circumst': 9,\n"," 'circumv': 3,\n"," 'citat': 1,\n"," 'cite': 1,\n"," 'citi': 4,\n"," 'civil': 1,\n"," 'claim': 10,\n"," 'clarif': 4,\n"," 'clarifi': 11,\n"," 'clariti': 2,\n"," 'clark': 1,\n"," 'class': 130,\n"," 'classic': 20,\n"," 'classif': 16,\n"," 'classifi': 12,\n"," 'classroom': 2,\n"," 'clau': 4,\n"," 'clausal': 1,\n"," 'clean': 3,\n"," 'clear': 11,\n"," 'clearanc': 1,\n"," 'clearli': 6,\n"," 'clebsch': 1,\n"," 'clenshaw': 6,\n"," 'cleric': 1,\n"," 'clerk': 2,\n"," 'client': 2,\n"," 'climat': 1,\n"," 'clinic': 8,\n"," 'clip': 17,\n"," 'cliqu': 5,\n"," 'clock': 10,\n"," 'close': 37,\n"," 'closer': 2,\n"," 'closest': 2,\n"," 'closet': 2,\n"," 'closur': 4,\n"," 'cloud': 2,\n"," 'clp': 3,\n"," 'clu': 6,\n"," 'club': 1,\n"," 'clue': 1,\n"," 'cluster': 17,\n"," 'clutter': 1,\n"," 'co': 1,\n"," 'coastlin': 1,\n"," 'coaxial': 1,\n"," 'cobol': 58,\n"," 'cobolin': 1,\n"," 'cock': 2,\n"," 'coda': 1,\n"," 'codasyl': 3,\n"," 'codd': 1,\n"," 'code': 233,\n"," 'coder': 2,\n"," 'codeword': 5,\n"," 'codifi': 1,\n"," 'coeffici': 57,\n"," 'coercion': 1,\n"," 'coexist': 1,\n"," 'coffman': 2,\n"," 'cognat': 1,\n"," 'cohe': 1,\n"," 'coher': 2,\n"," 'coif': 1,\n"," 'coin': 1,\n"," 'coincid': 3,\n"," 'coko': 5,\n"," 'cold': 5,\n"," 'colinear': 1,\n"," 'collat': 2,\n"," 'collect': 61,\n"," 'collector': 8,\n"," 'colleg': 16,\n"," 'colli': 6,\n"," 'collin': 2,\n"," 'color': 13,\n"," 'columbia': 1,\n"," 'column': 21,\n"," 'combat': 2,\n"," 'combin': 105,\n"," 'combinatori': 22,\n"," 'come': 10,\n"," 'comit': 5,\n"," 'comm': 8,\n"," 'command': 44,\n"," 'comment': 54,\n"," 'commentari': 1,\n"," 'commerci': 7,\n"," 'commiss': 2,\n"," 'commit': 1,\n"," 'committ': 25,\n"," 'common': 68,\n"," 'commonli': 20,\n"," 'commun': 91,\n"," 'communist': 2,\n"," 'commut': 1,\n"," 'compact': 27,\n"," 'compactifi': 2,\n"," 'compani': 5,\n"," 'companion': 3,\n"," 'compar': 140,\n"," 'comparison': 82,\n"," 'compass': 2,\n"," 'compat': 25,\n"," 'compen': 3,\n"," 'compet': 4,\n"," 'competit': 2,\n"," 'compil': 220,\n"," 'complement': 15,\n"," 'complementar': 2,\n"," 'complementari': 7,\n"," 'complet': 101,\n"," 'complex': 159,\n"," 'compli': 1,\n"," 'complic': 21,\n"," 'compo': 12,\n"," 'compon': 62,\n"," 'composit': 29,\n"," 'compound': 3,\n"," 'comprehen': 16,\n"," 'comprehend': 9,\n"," 'compress': 19,\n"," 'compressor': 2,\n"," 'compri': 3,\n"," 'compromi': 7,\n"," 'comput': 1326,\n"," 'con': 4,\n"," 'concaten': 9,\n"," 'concav': 3,\n"," 'conceiv': 4,\n"," 'concentr': 10,\n"," 'concept': 118,\n"," 'conceptu': 19,\n"," 'concern': 60,\n"," 'conci': 9,\n"," 'conclu': 26,\n"," 'conclud': 25,\n"," 'concord': 3,\n"," 'concret': 3,\n"," 'concurr': 20,\n"," 'conden': 6,\n"," 'condit': 131,\n"," 'conduc': 2,\n"," 'conduct': 20,\n"," 'confer': 6,\n"," 'confid': 2,\n"," 'confidenti': 4,\n"," 'configur': 34,\n"," 'confin': 7,\n"," 'confirm': 3,\n"," 'conflict': 17,\n"," 'confluent': 4,\n"," 'conform': 5,\n"," 'confront': 1,\n"," 'confu': 2,\n"," 'congress': 2,\n"," 'congruent': 1,\n"," 'congruenti': 5,\n"," 'conjectur': 4,\n"," 'conjug': 4,\n"," 'conjunct': 16,\n"," 'connect': 48,\n"," 'connot': 1,\n"," 'conquer': 1,\n"," 'consciou': 1,\n"," 'consecut': 12,\n"," 'consequ': 9,\n"," 'conserv': 1,\n"," 'consid': 214,\n"," 'considerd': 1,\n"," 'consist': 78,\n"," 'consol': 20,\n"," 'consolid': 3,\n"," 'constant': 38,\n"," 'constel': 1,\n"," 'constitu': 4,\n"," 'constitut': 10,\n"," 'constrain': 12,\n"," 'constraint': 45,\n"," 'construct': 166,\n"," 'consult': 1,\n"," 'consum': 17,\n"," 'consumm': 1,\n"," 'consumpt': 1,\n"," 'cont': 4,\n"," 'contact': 2,\n"," 'contactless': 1,\n"," 'contain': 18,\n"," 'contempl': 1,\n"," 'contemporari': 6,\n"," 'contend': 3,\n"," 'content': 32,\n"," 'context': 95,\n"," 'contigu': 7,\n"," 'contin': 1,\n"," 'continu': 70,\n"," 'contour': 28,\n"," 'contract': 3,\n"," 'contradict': 2,\n"," 'contradictori': 1,\n"," 'contrari': 1,\n"," 'contrast': 12,\n"," 'contribut': 19,\n"," 'control': 219,\n"," 'controversi': 1,\n"," 'conveni': 29,\n"," 'convent': 81,\n"," 'conver': 67,\n"," 'converg': 38,\n"," 'convert': 42,\n"," 'convex': 14,\n"," 'convey': 1,\n"," 'convict': 1,\n"," 'convinc': 1,\n"," 'convolut': 4,\n"," 'cooley': 2,\n"," 'cooper': 6,\n"," 'coordin': 35,\n"," 'cope': 4,\n"," 'copi': 18,\n"," ...}"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["# Primero, obtenemos las frecuencias de las palabras\n","\n","from collections import Counter\n","\n","dict_freq = dict(sorted(Counter(doc_words).items()))\n","dict_freq\n","\n","#print(c.items())"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["# separando las palabras por documento:\n","# text\n","from itertools import count\n","from operator import countOf\n","\n","\n","words = re.split(r\"\\n\", text)\n","doc_words_per_doc = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        doc_words_per_doc[i] = [x for x in split[2].split(\" \") if x]\n","    elif len(split) == 3:\n","        doc_words_per_doc[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","doc_words_per_doc[1]\n","\n","# contamos las frecuencias de cada término por documento\n","freq_per_doc = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_doc[d][t] = countOf(doc_words_per_doc[d], t)\n","        #print(t)\n","        \n","   \n","#{1: {\"premiliniari\": 1, \"report\": 3, ... , \"algorithm\": 0, ...}}"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["# dict_freq         -> vector q\n","# freq_per_doc[1]   -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Ahora lo mismo pero con el vocabulario reducido"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5061\n","4567\n"]}],"source":["import copy\n","filename = \"vocabulario_reducido_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","red_voc = file.read().split(\", \")\n","file.close()\n","red_voc = sorted(red_voc)\n","\n","new_voc = copy.copy(dict_freq)\n","#freq_per_doc_red = { key : value for (key, value) in dict_freq if key in red_voc }\n","del_keys = [ key for key in new_voc if key not in red_voc ]\n","for key in del_keys:\n","     del new_voc[key]\n","print(len(dict_freq))\n","print(len(new_voc))\n"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["freq_per_doc_red = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_doc_red[d][t] = countOf(doc_words_per_doc[d], t)\n","        \n","# new_voc           -> vector q reducido\n","# freq_per_doc_red  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries\n","### Vocabulario normal"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["64\n"]}],"source":["filename = \"querys_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","quer_words_per_quer = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    else:\n","        quer_words_per_quer[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","print(len(quer_words_per_quer))"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["freq_per_quer = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_quer[d][t] = countOf(quer_words_per_quer[d], t)\n","        #print(t)\n","\n","# dict_freq         -> vector q\n","# freq_per_quer     -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Vocabulario reducido"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["freq_per_quer_red = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_quer_red[d][t] = countOf(quer_words_per_quer[d], t)\n","\n","\n","# new_voc         -> vector q\n","# freq_per_quer_red     -> vector d"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(freq_per_quer[1][\"tss\"])\n","#print(freq_per_quer_red[1][\"tss\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Con tf-idf"]},{"cell_type":"markdown","metadata":{},"source":["Calcularemos primero un vector con los valores de frecuencia inversa del doc (idf)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["#primero, calculamos N_t (# de docs con el término)\n","from math import log\n","\n","\n","N_t = dict().fromkeys(dict_freq.keys())\n","N_t = { x : 0 for x in N_t}\n","for i in N_t.keys():\n","    for j in freq_per_doc.keys():\n","        if freq_per_doc[j][i] != 0:\n","            N_t[i] += 1\n"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"data":{"text/plain":["1.9089829173416073"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["# calculando idf_i\n","idf_i = dict().fromkeys(N_t.keys())\n","for i in idf_i.keys():\n","    idf_i[i] = log(doc_count / N_t[i]) + 1\n","idf_i['algorithm']"]},{"cell_type":"markdown","metadata":{},"source":["Obteniendo las matrices"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["dict_freq_2 = dict().fromkeys(dict_freq.keys())\n","for i in dict_freq.keys():\n","    dict_freq_2[i] = dict_freq[i] * idf_i[i]\n","    \n","freq_per_doc_2 = dict()\n","for i in freq_per_doc.keys():\n","    freq_per_doc_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_doc_2[i][j] = freq_per_doc[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_doc_2   -> vector d"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["new_voc_2 = dict().fromkeys(new_voc.keys())\n","for i in new_voc_2.keys():\n","    new_voc_2[i] = new_voc[i] * idf_i[i]\n","    \n","freq_per_doc_red_2 = dict()\n","for i in freq_per_doc_red.keys():\n","    freq_per_doc_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_doc_red_2[i][j] = freq_per_doc_red[i][j] * idf_i[j]\n","        \n","# new_voc_2           -> vector q reducido\n","# freq_per_doc_red_2  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":[" \n","freq_per_quer_2 = dict()\n","for i in freq_per_quer.keys():\n","    freq_per_quer_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_quer_2[i][j] = freq_per_quer[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_quer_2     -> vector d"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["freq_per_quer_red_2 = dict()\n","for i in freq_per_quer_red.keys():\n","    freq_per_quer_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_quer_red_2[i][j] = freq_per_quer_red[i][j] * idf_i[j]\n","\n","# new_voc_2         -> vector q\n","# freq_per_quer_red_2     -> vector d"]}],"metadata":{"colab":{"name":"p1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"8bac3ed83652cd3fb2be2ab4720f126b858190479445937cf7f7241afc53dfb0"}}},"nbformat":4,"nbformat_minor":0}
