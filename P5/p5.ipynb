{"cells":[{"cell_type":"markdown","metadata":{"id":"F4qwv6V8DiGv"},"source":["# Laboratorio 5\n","\n","1. Utilice los archivos pre-procesados que obtuvo en la práctica 3, mismos que almacenan la colección CACM y los queries.\n","2. Si en la práctica tres generó el vocabulario sin ordenarlo alfabéticamente ordénelo, tanto el normal como el reducido.\n","3. Represente cada documento empleando el modelo vectorial y el esquema de pesado tf. Recuerde que la dimensión de los vectores será igual al tamaño del vocabulario que obtuvo en la práctica 4. Trabajen con el vocabulario sin reducir y después reducido. Guarda las matrices generada, que representa los documentos de la colección, para utilizarla en prácticas posteriores.\n","4. Utiliza el archivo de queries y represéntelos también empleando el modelo vectorial y esquema de pesado tf. Con vocabulario normal y reducido. Guarda las matrices generadas."]},{"cell_type":"markdown","metadata":{},"source":["Primero extraemos las palabras de documentos_final.txt\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['preliminari', 'report', 'intern', 'algebra', 'languag', 'extract', 'root', 'repeat', 'subtract', 'digit']\n","3204\n"]}],"source":["filename = \"documentos_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","doc_words = \"\"\n","doc_count = 0\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        #doc_data[i]['d_num'] = split[0]\n","        doc_words += split[2] + \" \"\n","    elif len(split) == 3:\n","        doc_words += split[1] + \" \"\n","    doc_count += 1\n","doc_words = [x for x in (doc_words).split(\" \") if x]\n","print(doc_words[:10])\n","print(doc_count)"]},{"cell_type":"markdown","metadata":{},"source":["Luego, importamos el vocabulario creado en la práctica 4."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['1604a', '16th', '1970', '2', '263a', '2f', '2k', '2m', '2n', '32k', '3n', '40k', '44x', '473l', '4k', '4n', '4x16', '5n', '64k', '7th', '8k', '8n', '9n', 'a', 'a1', 'a2', 'a418', 'a419', 'a420', 'a421', 'a422', 'a423', 'a424', 'a425', 'a426', 'a427', 'a428', 'a429', 'a430', 'a431', 'a432', 'a433', 'a434', 'a435', 'a436', 'a437', 'a438', 'a439', 'a440', 'a441', 'a442', 'a443', 'a444', 'a445', 'a446', 'a447', 'a448', 'a449', 'a450', 'a451', 'a452', 'a453', 'a454', 'a455', 'a456', 'a457', 'a458', 'a459', 'a460', 'a461', 'a462', 'a463', 'a464', 'a465', 'a466', 'a467', 'a468', 'a469', 'a470', 'a471', 'a472', 'a473', 'a474', 'a475', 'a476', 'a477', 'a478', 'a479', 'a480', 'a481', 'a482', 'a483', 'a484', 'a485', 'a486', 'a487', 'aaron', 'abac', 'abacu', 'abandon']\n"]}],"source":["filename = \"vocabulario_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","vocabulary = file.read().split(\", \")\n","file.close()\n","vocabulary = sorted(vocabulary)\n","print(vocabulary[:100])"]},{"cell_type":"markdown","metadata":{},"source":["Ahora, comenzamos los cálculos para la representación del vocabulario por medio del modelo vectorial."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5178\n"]}],"source":["# Primero, obtenemos las frecuencias de las palabras\n","\n","from collections import Counter\n","from operator import countOf\n","dict_freq = dict().fromkeys(vocabulary)\n","dict_freq = { x : 0 for x in dict_freq}\n","#for v in vocabulary:\n","#    dict_freq[v] = countOf(doc_words, v)\n","#dict_freq = dict(sorted(Counter(doc_words).items()))\n","for w in doc_words:\n","    dict_freq[w] = dict_freq[w] + 1\n","print(len(dict_freq.items()))\n","#print(c.items())"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# separando las palabras por documento:\n","# text\n","from itertools import count\n","from operator import countOf\n","\n","\n","words = re.split(r\"\\n\", text)\n","doc_words_per_doc = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        doc_words_per_doc[i] = [x for x in split[2].split(\" \") if x]\n","    elif len(split) == 3:\n","        doc_words_per_doc[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","doc_words_per_doc[1]\n","\n","# contamos las frecuencias de cada término por documento\n","freq_per_doc = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_doc[d][t] = countOf(doc_words_per_doc[d], t)\n","        #print(t)\n","        \n","   \n","#{1: {\"premiliniari\": 1, \"report\": 3, ... , \"algorithm\": 0, ...}} -> formato del diccionario\n","# dict_freq         -> vector q\n","# freq_per_doc[1]   -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Ahora lo mismo pero con el vocabulario reducido"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5178\n","4670\n"]}],"source":["import copy\n","filename = \"vocabulario_reducido_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","red_voc = file.read().split(\", \")\n","file.close()\n","red_voc = sorted(red_voc)\n","\n","new_voc = copy.copy(dict_freq)\n","#freq_per_doc_red = { key : value for (key, value) in dict_freq if key in red_voc }\n","del_keys = [ key for key in new_voc if key not in red_voc ]\n","for key in del_keys:\n","     del new_voc[key]\n","print(len(dict_freq))\n","print(len(new_voc))\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["freq_per_doc_red = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_doc_red[d][t] = countOf(doc_words_per_doc[d], t)\n","        \n","# new_voc           -> vector q reducido\n","# freq_per_doc_red  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries\n","### Vocabulario normal"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["64\n"]}],"source":["filename = \"querys_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","quer_words_per_quer = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    else:\n","        quer_words_per_quer[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","print(len(quer_words_per_quer))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["freq_per_quer = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_quer[d][t] = countOf(quer_words_per_quer[d], t)\n","        #print(t)\n","\n","# dict_freq         -> vector q\n","# freq_per_quer     -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Vocabulario reducido"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["freq_per_quer_red = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_quer_red[d][t] = countOf(quer_words_per_quer[d], t)\n","\n","\n","# new_voc         -> vector q\n","# freq_per_quer_red     -> vector d"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(freq_per_quer[1][\"tss\"])\n","#print(freq_per_quer_red[1][\"tss\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Con tf-idf"]},{"cell_type":"markdown","metadata":{},"source":["Calcularemos primero un vector con los valores de frecuencia inversa del doc (idf)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["#primero, calculamos N_t (# de docs con el término)\n","from math import log\n","\n","\n","N_t = dict().fromkeys(dict_freq.keys())\n","N_t = { x : 0 for x in N_t}\n","for i in N_t.keys():\n","    for j in freq_per_doc.keys():\n","        if freq_per_doc[j][i] != 0:\n","            N_t[i] += 1\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["1.9089829173416073"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# calculando idf_i\n","idf_i = dict().fromkeys(N_t.keys())\n","for i in idf_i.keys():\n","    idf_i[i] = 0\n","    if N_t[i]:\n","        idf_i[i] = log(doc_count / N_t[i]) + 1\n","\n","idf_i['algorithm']"]},{"cell_type":"markdown","metadata":{},"source":["Obteniendo las matrices"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["dict_freq_2 = dict().fromkeys(dict_freq.keys())\n","for i in dict_freq.keys():\n","    dict_freq_2[i] = dict_freq[i] * idf_i[i]\n","    \n","freq_per_doc_2 = dict()\n","for i in freq_per_doc.keys():\n","    freq_per_doc_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_doc_2[i][j] = freq_per_doc[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_doc_2   -> vector d"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["new_voc_2 = dict().fromkeys(new_voc.keys())\n","for i in new_voc_2.keys():\n","    new_voc_2[i] = new_voc[i] * idf_i[i]\n","    \n","freq_per_doc_red_2 = dict()\n","for i in freq_per_doc_red.keys():\n","    freq_per_doc_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_doc_red_2[i][j] = freq_per_doc_red[i][j] * idf_i[j]\n","        \n","# new_voc_2           -> vector q reducido\n","# freq_per_doc_red_2  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":[" \n","freq_per_quer_2 = dict()\n","for i in freq_per_quer.keys():\n","    freq_per_quer_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_quer_2[i][j] = freq_per_quer[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_quer_2     -> vector d"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["freq_per_quer_red_2 = dict()\n","for i in freq_per_quer_red.keys():\n","    freq_per_quer_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_quer_red_2[i][j] = freq_per_quer_red[i][j] * idf_i[j]\n","\n","# new_voc_2         -> vector q\n","# freq_per_quer_red_2     -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["### Guardando los resultados a 8 archivos distintos:\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# 1 vocabulario normal, documentos\n","# dict_freq         -> vector q\n","# freq_per_doc[1]   -> vector d\n","import json\n","\n","#with open('Resultados\\\\doc_q_normal.txt', 'w') as f:\n","#    f.write(json.dumps(dict_freq))\n","#    f.close()\n","with open('Resultados\\\\doc_d_normal.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc))\n","    f.close()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# 2. vocabulario reducido, documentos\n","#with open('Resultados\\\\doc_q_red.txt', 'w') as f:\n","#    f.write(json.dumps(new_voc))\n","#    f.close()\n","with open('Resultados\\\\doc_d_red.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc_red))\n","    f.close()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# 3. vocabulario normal, queries\n","#with open('Resultados\\\\doc_q_red.txt', 'w') as f:\n","#    f.write(json.dumps(new_voc))\n","#    f.close()\n","with open('Resultados\\\\quer_d_normal.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer))\n","    f.close()"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# 4. vocabulario reducido, queries\n","with open('Resultados\\\\quer_d_red.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer_red))\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 5. voc normal, "]}],"metadata":{"colab":{"name":"p1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"8bac3ed83652cd3fb2be2ab4720f126b858190479445937cf7f7241afc53dfb0"}}},"nbformat":4,"nbformat_minor":0}
