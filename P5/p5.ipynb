{"cells":[{"cell_type":"markdown","metadata":{"id":"F4qwv6V8DiGv"},"source":["# Laboratorio 5\n","\n","1. Utilice los archivos pre-procesados que obtuvo en la práctica 3, mismos que almacenan la colección CACM y los queries.\n","2. Si en la práctica tres generó el vocabulario sin ordenarlo alfabéticamente ordénelo, tanto el normal como el reducido.\n","3. Represente cada documento empleando el modelo vectorial y el esquema de pesado tf. Recuerde que la dimensión de los vectores será igual al tamaño del vocabulario que obtuvo en la práctica 4. Trabajen con el vocabulario sin reducir y después reducido. Guarda las matrices generada, que representa los documentos de la colección, para utilizarla en prácticas posteriores.\n","4. Utiliza el archivo de queries y represéntelos también empleando el modelo vectorial y esquema de pesado tf. Con vocabulario normal y reducido. Guarda las matrices generadas."]},{"cell_type":"markdown","metadata":{},"source":["Primero extraemos las palabras de documentos_final.txt\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['preliminari', 'report', 'intern', 'algebra', 'languag', 'extract', 'root', 'repeat', 'subtract', 'digit']\n","3204\n"]}],"source":["filename = \"documentos_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","doc_words = \"\"\n","doc_count = 0\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        #doc_data[i]['d_num'] = split[0]\n","        doc_words += split[1] + \" \" # titulo\n","        doc_words += split[2] + \" \" # texto\n","    elif len(split) == 3 or len(split) == 2:\n","        doc_words += split[1] + \" \" # titulo\n","    doc_count += 1\n","doc_words = [x for x in (doc_words).split(\" \") if x]\n","print(doc_words[:10])\n","print(doc_count)"]},{"cell_type":"markdown","metadata":{},"source":["Luego, importamos el vocabulario creado en la práctica 4."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5178\n"]}],"source":["filename = \"vocabulario_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","vocabulary = file.read().split(\", \")\n","file.close()\n","vocabulary = sorted(vocabulary)\n","print(len(vocabulary))"]},{"cell_type":"markdown","metadata":{},"source":["Ahora, comenzamos los cálculos para la representación del vocabulario por medio del modelo vectorial."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5178\n"]},{"data":{"text/plain":["16"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Primero, obtenemos las frecuencias de las palabras\n","\n","from collections import Counter\n","from operator import countOf\n","dict_freq = dict().fromkeys(vocabulary)\n","dict_freq = { x : 0 for x in dict_freq}\n","#for v in vocabulary:\n","#    dict_freq[v] = countOf(doc_words, v)\n","#dict_freq = dict(sorted(Counter(doc_words).items()))\n","for w in doc_words:\n","    dict_freq[w] = dict_freq[w] + 1\n","print(len(dict_freq.items()))\n","#print(c.items())\n","dict_freq['preliminari']"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# separando las palabras por documento:\n","# text\n","from itertools import count\n","from operator import countOf\n","\n","\n","words = re.split(r\"\\n\", text)\n","doc_words_per_doc = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        doc_words_per_doc[i] = [x for x in split[2].split(\" \") if x]\n","    elif len(split) == 3:\n","        doc_words_per_doc[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","doc_words_per_doc[1]\n","\n","# contamos las frecuencias de cada término por documento\n","freq_per_doc = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_doc[d][t] = countOf(doc_words_per_doc[d], t)\n","        #print(t)\n","        \n","   \n","#{1: {\"premiliniari\": 1, \"report\": 3, ... , \"algorithm\": 0, ...}} -> formato del diccionario\n","# dict_freq         -> vector q\n","# freq_per_doc[1]   -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Ahora lo mismo pero con el vocabulario reducido"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5178\n","4670\n"]}],"source":["import copy\n","filename = \"vocabulario_reducido_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","red_voc = file.read().split(\", \")\n","file.close()\n","red_voc = sorted(red_voc)\n","\n","new_voc = copy.copy(dict_freq)\n","#freq_per_doc_red = { key : value for (key, value) in dict_freq if key in red_voc }\n","del_keys = [ key for key in new_voc if key not in red_voc ]\n","for key in del_keys:\n","     del new_voc[key]\n","print(len(dict_freq))\n","print(len(new_voc))\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["freq_per_doc_red = dict()\n","for d in doc_words_per_doc.keys():\n","    freq_per_doc_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_doc_red[d][t] = countOf(doc_words_per_doc[d], t)\n","        \n","# new_voc           -> vector q reducido\n","# freq_per_doc_red  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries\n","### Vocabulario normal"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["64\n"]}],"source":["filename = \"querys_trunc.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","quer_words_per_quer = dict()\n","i = 1\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    index = str(i)\n","    if len(split) == 1:\n","        continue\n","    else:\n","        quer_words_per_quer[i] = [x for x in split[1].split(\" \") if x]\n","    i += 1\n","print(len(quer_words_per_quer))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["freq_per_quer = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer[d] = dict().fromkeys(dict_freq.keys())\n","    for t in dict_freq.keys():\n","        freq_per_quer[d][t] = countOf(quer_words_per_quer[d], t)\n","        #print(t)\n","\n","# dict_freq         -> vector q\n","# freq_per_quer     -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["Vocabulario reducido"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["freq_per_quer_red = dict()\n","for d in quer_words_per_quer.keys():\n","    freq_per_quer_red[d] = dict().fromkeys(new_voc.keys())\n","    for t in new_voc.keys():\n","        freq_per_quer_red[d][t] = countOf(quer_words_per_quer[d], t)\n","\n","\n","# new_voc         -> vector q\n","# freq_per_quer_red     -> vector d"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(freq_per_quer[1][\"tss\"])\n","#print(freq_per_quer_red[1][\"tss\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Con tf-idf"]},{"cell_type":"markdown","metadata":{},"source":["Calcularemos primero un vector con los valores de frecuencia inversa del doc (idf)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["#primero, calculamos N_t (# de docs con el término)\n","from math import log\n","\n","\n","N_t = dict().fromkeys(dict_freq.keys())\n","N_t = { x : 0 for x in N_t}\n","for i in N_t.keys():\n","    for j in freq_per_doc.keys():\n","        if freq_per_doc[j][i] != 0:\n","            N_t[i] += 1\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["1.9089829173416073"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# calculando idf_i\n","idf_i = dict().fromkeys(N_t.keys())\n","for i in idf_i.keys():\n","    idf_i[i] = 0\n","    if N_t[i]:\n","        idf_i[i] = log(doc_count / N_t[i]) + 1\n","\n","idf_i['algorithm']"]},{"cell_type":"markdown","metadata":{},"source":["Obteniendo las matrices"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["dict_freq_2 = dict().fromkeys(dict_freq.keys())\n","for i in dict_freq.keys():\n","    dict_freq_2[i] = dict_freq[i] * idf_i[i]\n","    \n","freq_per_doc_2 = dict()\n","for i in freq_per_doc.keys():\n","    freq_per_doc_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_doc_2[i][j] = freq_per_doc[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_doc_2   -> vector d"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["new_voc_2 = dict().fromkeys(new_voc.keys())\n","for i in new_voc_2.keys():\n","    new_voc_2[i] = new_voc[i] * idf_i[i]\n","    \n","freq_per_doc_red_2 = dict()\n","for i in freq_per_doc_red.keys():\n","    freq_per_doc_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_doc_red_2[i][j] = freq_per_doc_red[i][j] * idf_i[j]\n","        \n","# new_voc_2           -> vector q reducido\n","# freq_per_doc_red_2  -> matriz d reducido"]},{"cell_type":"markdown","metadata":{},"source":["## Con queries"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":[" \n","freq_per_quer_2 = dict()\n","for i in freq_per_quer.keys():\n","    freq_per_quer_2[i] = dict().fromkeys(dict_freq.keys())\n","    for j in dict_freq.keys():\n","        freq_per_quer_2[i][j] = freq_per_quer[i][j] * idf_i[j]\n","        \n","# dict_freq_2         -> vector q\n","# freq_per_quer_2     -> vector d"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["freq_per_quer_red_2 = dict()\n","for i in freq_per_quer_red.keys():\n","    freq_per_quer_red_2[i] = dict().fromkeys(new_voc.keys())\n","    for j in new_voc.keys():\n","        freq_per_quer_red_2[i][j] = freq_per_quer_red[i][j] * idf_i[j]\n","\n","# new_voc_2         -> vector q\n","# freq_per_quer_red_2     -> vector d"]},{"cell_type":"markdown","metadata":{},"source":["### Guardando los resultados a 8 archivos distintos:\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# 1 vocabulario normal, documentos\n","# dict_freq         -> vector q\n","# freq_per_doc[1]   -> vector d\n","import json\n","\n","#with open('Resultados\\\\doc_q_normal.txt', 'w') as f:\n","#    f.write(json.dumps(dict_freq))\n","#    f.close()\n","with open('Resultados\\\\doc_d_normal.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc))\n","    f.close()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# 2. vocabulario reducido, documentos\n","#with open('Resultados\\\\doc_q_red.txt', 'w') as f:\n","#    f.write(json.dumps(new_voc))\n","#    f.close()\n","with open('Resultados\\\\doc_d_red.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc_red))\n","    f.close()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# 3. vocabulario normal, queries\n","with open('Resultados\\\\quer_d_normal.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer))\n","    f.close()"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# 4. vocabulario reducido, queries\n","with open('Resultados\\\\quer_d_red.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer_red))\n","    f.close()"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["# 5. voc normal, docs, tf-idf\n","with open('Resultados\\\\doc_d_normal-2.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc_2))\n","    f.close()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# 6. voc red, docs, tf-idf\n","with open('Resultados\\\\doc_d_red-2.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_doc_red_2))\n","    f.close()"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# 7. vocabulario normal, queries, tf-idf\n","with open('Resultados\\\\quer_d_normal-2.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer_2))\n","    f.close()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# 8. vocabulario reducido, queries, tf-idf\n","with open('Resultados\\\\quer_d_red-2.txt', 'w') as f:\n","    f.write(json.dumps(freq_per_quer_red_2))\n","    f.close()"]}],"metadata":{"colab":{"name":"p1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"8bac3ed83652cd3fb2be2ab4720f126b858190479445937cf7f7241afc53dfb0"}}},"nbformat":4,"nbformat_minor":0}
