{"cells":[{"cell_type":"markdown","metadata":{"id":"F4qwv6V8DiGv"},"source":["# Laboratorio 4"]},{"cell_type":"markdown","metadata":{"id":"PudnwJzRDiG7"},"source":["1. Utilice el archivo generado en el laboratorio 3 con los documentos de la colección de CACM preprocesados.\n","1. Obtengan el vocabulario de la colección utilizando truncamiento. Impriman la longitud del vocabulario. Después de observar el vocabulario resultante, proponga mecanismos para reducirlo. Codifique los mecanismos propuestos. Escriba la longitud del vocabulario reducido. Las longitudes deberán escribirse a pantalla\n","1. Escriban los vocabularios finales a archivos vocabulario.txt y vocabulario_reducido.tx\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Primero extraemos las palabras de documentos_final.txt\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" preliminari report intern algebra languag   extract root repeat subtract digit comput   techniqu depart matrix program scheme   glossari comput engin program terminolog  squar root approxim   comput inspect procedur   glossari comput engin program terminolog  equiv transform program scheme   propo uncol   glossari comput engin program terminolog  problem program commun chang machin propo solut part   error estim rung kutta procedur   glossari comput engin program terminolog  problem program commun chang machin propo solut part   recur curv fit techniqu   secant modif newton method   program arithmet oper   simpl automat code system   glossari comput engin program terminolog  techniqu discuss appli iter procedur solut equat accel rate converg iter converg induc converg iter diverg illustr   accel converg iter process   algebra formul flow diagram   unusu applic depart automat implement comput logic   binari truth function oper decim comput extract command   improv decim redund check   \n"]}],"source":["filename = \"documentos_final.txt\"\n","file = open(filename, 'rt',encoding='utf-8-sig')\n","text = file.read()\n","file.close()\n","import re\n","words = re.split(r\"\\n\", text)\n","doc_words = \"\"\n","i = 0\n","for w in words:\n","    split = re.split(r\"\\|\", w)\n","    if len(split) == 1:\n","        continue\n","    elif len(split) == 4:\n","        #doc_data[i]['d_num'] = split[0]\n","        doc_words += split[2] + \" \"\n","    doc_words += split[1] + \" \"\n","    i = i + 1\n","print(doc_words[:1000])"]},{"cell_type":"markdown","metadata":{},"source":["Unimos y separamos todas las palabras (omitiendo vacías)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['preliminary', 'report', 'international', 'algebraic', 'language', 'extraction', 'root', 'repeated', 'subtraction', 'digital', 'computer', 'technique', 'department', 'matrix', 'program', 'scheme', 'glossary', 'computer', 'engineering', 'programming', 'terminology', 'square', 'root', 'approximation', 'computer', 'inspection', 'procedure', 'glossary', 'computer', 'engineering', 'programming', 'terminology', 'equivalence', 'transformation', 'program', 'scheme', 'proposal', 'uncol', 'glossary', 'computer', 'engineering', 'programming', 'terminology', 'problem', 'programming', 'communication', 'changing', 'machine', 'proposed', 'solution', 'part', 'error', 'estimation', 'runge', 'kutta', 'procedure', 'glossary', 'computer', 'engineering', 'programming', 'terminology', 'problem', 'programming', 'communication', 'changing', 'machine', 'proposed', 'solution', 'part', 'recursive', 'curve', 'fitting', 'technique', 'secant', 'modification', 'newton', 'method', 'programming', 'arithmetic', 'operation', 'simple', 'automatic', 'coding', 'system', 'glossary', 'computer', 'engineering', 'programming', 'terminology', 'technique', 'discussed', 'applied', 'iterative', 'procedure', 'solution', 'equation', 'accelerates', 'rate', 'convergence', 'iteration']\n","90862\n"]}],"source":["all_words = [x for x in (doc_words).split(\" \") if x]\n","print(all_words[:100])\n","print(str(len(all_words)))"]},{"cell_type":"markdown","metadata":{},"source":["Ahora buscaremos reducir el vocabulario existente"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('preliminary', 'report', 'international', 'algebraic', 'language', 'extraction', 'root', 'repeated', 'subtraction', 'digital')\n","(19, 95, 7, 67, 815, 10, 66, 7, 9, 133)\n","7560\n"]}],"source":["from collections import Counter\n","\n","c = Counter(all_words)\n","#print(c.items())\n","labels, values = zip(*c.items())\n","print(labels[:10])\n","print(values[:10])\n","print(len(labels))"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 6.0, 10.0, 24.0]\n"]}],"source":["# vemos los datos en deciles\n","import statistics\n","deciles = statistics.quantiles(values, n=10)\n","print(deciles)"]},{"cell_type":"markdown","metadata":{},"source":["Viendo la distribución de los datos en deciles, vemos que la casi mitad de las palabras del vocabulario aparecen en frecuencias cercanas a 1 y 2.\n","\n","Se optará por eliminar solamente las palabras con 1 ocurrencia en el vocabulario."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4712\n","We deleted 2848 words from the vocabulary\n"]}],"source":["reduced_words = {k: v for k, v in c.items() if v != 1}\n","print(len(reduced_words))\n","print(\"We deleted \" + str(len(labels) - len(reduced_words)) + \" words from the vocabulary\")"]},{"cell_type":"markdown","metadata":{},"source":["Vemos que se eliminaron 2848 palabras, que a mi consideración fueron demasiadas, seguiremos revisando el vocabulario existente.\n","\n","Analizaremos la distribución del largo de los términos del vocabulario."]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["[(1, 26),\n"," (2, 778),\n"," (3, 2333),\n"," (4, 9255),\n"," (5, 8942),\n"," (6, 12425),\n"," (7, 13222),\n"," (8, 13284),\n"," (9, 12396),\n"," (10, 7096),\n"," (11, 6079),\n"," (12, 2330),\n"," (13, 1336),\n"," (14, 998),\n"," (15, 191),\n"," (16, 133),\n"," (17, 23),\n"," (18, 11),\n"," (19, 1),\n"," (20, 3)]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["len_freq_dict = dict()\n","for w in all_words:\n","    len_freq_dict[len(w)] = 0\n","for w in all_words:\n","    len_freq_dict[len(w)] = len_freq_dict[len(w)] + 1\n","len_freq_list = (sorted(len_freq_dict.items()))\n","len_freq_list"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["13.638264621073718\n"]}],"source":["x = len_freq_list[0][1] +len_freq_list[1][1] + len_freq_list[2][1] + len_freq_list[3][1]\n","print(str(x / len(all_words) * 100))"]},{"cell_type":"markdown","metadata":{},"source":["Aquí ya vemos que las palabras con 1, 2, 3 y 4 caracteres que existen en el vocabulario conforman el 13.6% del diccionario, por lo que estas serán las que se eliminarán"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We deleted 1091 words from the vocabulary\n","6469\n"]}],"source":["reduced_words = {k: v for k, v in c.items() if len(k) > 4}\n","print(\"We deleted \" + str(len(labels) - len(reduced_words)) + \" words from the vocabulary\")\n","print(len(reduced_words))"]},{"cell_type":"markdown","metadata":{},"source":["Por último, las palabras serán escritas a vocabulario.txt y vocabulario_reducido.txt"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["voc = ', '.join(list(labels))\n","reduced_voc = ', '.join(list(reduced_words.keys()))\n","with open('vocabulario.txt', 'w') as f:\n","    f.write(voc)\n","    f.close()\n","with open('vocabulario_reducido.txt', 'w') as f:\n","    f.write(reduced_voc)\n","    f.close()"]},{"cell_type":"markdown","metadata":{},"source":["## Test con truncamiento\n","Todo el código anterior procesa un vocabulario creado con lematización, como prueba, se realizó el mismo procesamiento con un vocabulario creado a partir de truncamiento con el algoritmo de porter. Este se encuentra en los archivos **documentos_trunc.txt** y **querys_trunc.txt**\n","\n","El script se ejecuta en una terminal en este directorio de la forma: \n",">\\>python p4.py\n","\n","Los resultados fueron los siguientes:\n","\n","![Resultados procesamiento vocabulario con truncamiento](test_trunc.png)\n","\n","Vemos que el vocabulario inicial comienza en 5250 palabras, ¡una reducción de más de 2000 palabras en contraste con el vocabulario con lematización!\n","\n","Se observa también que si buscamos eliminar palabras con 1 sola ocurrencia nos topamos con la misma situación que con lematización, el vocabulario se reduciría demasiado (más de la mitad), por lo que la opción de eliminar por largo será la más adecuada en este caso.\n","\n","Podemos solamente omitr palabras de largo = {1,2,3} y no de 4 también, eliminando el 9.85% del vocabulario de esta forma y teniendo un vocabulario final de 4670 términos."]}],"metadata":{"colab":{"name":"p1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"8bac3ed83652cd3fb2be2ab4720f126b858190479445937cf7f7241afc53dfb0"}}},"nbformat":4,"nbformat_minor":0}
