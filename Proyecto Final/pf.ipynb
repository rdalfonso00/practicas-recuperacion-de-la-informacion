{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final - Recuperación de la Información \n",
    "## Otoño 2023 \n",
    "## Benemérita Universidad Autónoma de Puebla\n",
    "### Alfonso Reyes D'Elia \n",
    "---\n",
    "### __Planteamiento:__\n",
    "1. Elegir alguno de los corpus localizados en Teams en la carpeta de Proyecto. Encontrarán ahí documentos para entrenamiento y pruebas o en su defecto tome el 80 por ciento de datos para entrenar y el 20 para probar.\n",
    "2. Preprocesen los documentos para obtener la representación vectorial de los mismos.\n",
    "3. Utilicen Weka o Python para clasificar los textos, experimenten clasificando con tf con tf-idf y reduciendo el vocabulario\n",
    "4. Generen su reporte en el mismo formato de las prácticas reportando la exactitud, precisión, recuerdo y medida F obtenidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocesamiento\n",
    "* Extraeremos la información del dataset: blog-gender-dataset.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "blog_df = pd.read_excel(\"assets/blog-gender-dataset.xlsx\", sheet_name=\"training\" ,\n",
    "                        header=None, usecols=[0,1], names=[\"text\",\"gender\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Long time no see. Like always I was rewriting...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text gender\n",
       "0      Long time no see. Like always I was rewriting...      M\n",
       "1      Guest Demo: Eric Iverson’s Itty Bitty Search\\...      M\n",
       "2     Who moved my Cheese???   The world has been de...      M\n",
       "3      Yesterday I attended a biweekly meeting of an...      M\n",
       "4      Liam is nothing like Natalie. Natalie never w...      F\n",
       "...                                                 ...    ...\n",
       "3227  It was a scavenger style race with checkpoints...      M\n",
       "3228  Finally! I got a full day's work done. Almost ...      F\n",
       "3229  At the height of laughter, the universe is flu...      M\n",
       "3230  I like birds, especially woodpeckers and MOST ...      M\n",
       "3231  Oh friends, it's finally here! I thought the m...      F\n",
       "\n",
       "[3232 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(blog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Antes de hacer cualquier procesamiento, se tiene que generar el modelado del corpus según el modelo vec\n",
    "Primero, necesitamos obtener el vocabulario, para ello, se extrae todo este de la columna _text_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Long time no see. Like always I was rewriting it from scratch a couple of times. But nevertheless i\n"
     ]
    }
   ],
   "source": [
    "all_text = \"\"\n",
    "for i in blog_df.index:\n",
    "    all_text = all_text + str(blog_df['text'][i])\n",
    "print(all_text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tokenizes the text\n",
    "import string, re\n",
    "\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def tokenize_text(text: str, re_punc):\n",
    "    text_lower = text.lower()\n",
    "    words = re.split(r'\\W+',text_lower)\n",
    "    return [re_punc.sub(\"\",w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize_text(all_text, re_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'long', 'time', 'no', 'see', 'like', 'always', 'i', 'was', 'rewriting']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eliminaremos stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\poncho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'long', 'time', 'see', 'like', 'always', 'rewriting', 'scratch', 'couple', 'times', 'nevertheless', 'still', 'java', 'uses', 'metropolis', 'sampling', 'help', 'poor', 'path', 'tracing', 'converge', 'btw', 'mlt', 'yesterday', 'evening', '2', 'beers', 'ballmer', 'peak', 'altough', 'implementation', 'still', 'fresh', 'easily', 'outperforms', 'standard', 'path', 'tracing', 'seen', 'especially', 'difficult', 'caustics', 'involved', 'implemented', 'spectral', 'rendering', 'easy', 'actually', 'cause', 'computations', 'wavelengths', 'linear', 'like', 'rgb', 'realised', 'even', 'feel', 'physically', 'correct', 'whats', 'point', '3d', 'applications', 'operating', 'rgb', 'color', 'space', 'cant', 'represent', 'rgb', 'color', 'spectrum', 'interchangeably', 'approximate', 'long', 'running', 'physical', 'simulation', 'something', 'see', 'benefits', 'please', 'correct', 'wrong', 'thus', 'abandoned', 'guest', 'demo', 'eric', 'iverson', 'itty', 'bitty', 'search', 'february', '16th', '2010', 'daniel', 'tunkelang', 'respond', 'back']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Ahora, ya se pueden importar estas palabras sin mucho problema\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Para eliminar las palabras vacías, se hace uso de las operaciones de list comprehension de python\n",
    "words_clean = [x for x in words if x not in stop_words]\n",
    "print(words_clean[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora, eliminar dígitos y palabras nulas _\"\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'time', 'see', 'like', 'always', 'rewriting', 'scratch', 'couple', 'times', 'nevertheless', 'still', 'java', 'uses', 'metropolis', 'sampling', 'help', 'poor', 'path', 'tracing', 'converge', 'btw', 'mlt', 'yesterday', 'evening', 'beers', 'ballmer', 'peak', 'altough', 'implementation', 'still', 'fresh', 'easily', 'outperforms', 'standard', 'path', 'tracing', 'seen', 'especially', 'difficult', 'caustics', 'involved', 'implemented', 'spectral', 'rendering', 'easy', 'actually', 'cause', 'computations', 'wavelengths', 'linear', 'like', 'rgb', 'realised', 'even', 'feel', 'physically', 'correct', 'whats', 'point', '3d', 'applications', 'operating', 'rgb', 'color', 'space', 'cant', 'represent', 'rgb', 'color', 'spectrum', 'interchangeably', 'approximate', 'long', 'running', 'physical', 'simulation', 'something', 'see', 'benefits', 'please', 'correct', 'wrong', 'thus', 'abandoned', 'guest', 'demo', 'eric', 'iverson', 'itty', 'bitty', 'search', 'february', '16th', 'daniel', 'tunkelang', 'respond', 'back', 'vacation', 'still', 'digging']\n",
      "700726\n"
     ]
    }
   ],
   "source": [
    "words_filtered = [x for x in words_clean if not x.isdigit() and x]\n",
    "print(words_filtered[:100])\n",
    "print(len(words_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El corpus está compuesto originalmente por 700726, se procederá a determinar el vocabulario (contando las palabras), no sin antes primero aplicar truncamiento (con porter-stemmer) al conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'time', 'see', 'like', 'alway', 'rewrit', 'scratch', 'coupl', 'time', 'nevertheless', 'still', 'java', 'use', 'metropoli', 'sampl', 'help', 'poor', 'path', 'trace', 'converg', 'btw', 'mlt', 'yesterday', 'even', 'beer', 'ballmer', 'peak', 'altough', 'implement', 'still', 'fresh', 'easili', 'outperform', 'standard', 'path', 'trace', 'seen', 'especi', 'difficult', 'caustic', 'involv', 'implement', 'spectral', 'render', 'easi', 'actual', 'caus', 'comput', 'wavelength', 'linear', 'like', 'rgb', 'realis', 'even', 'feel', 'physic', 'correct', 'what', 'point', '3d', 'applic', 'oper', 'rgb', 'color', 'space', 'cant', 'repres', 'rgb', 'color', 'spectrum', 'interchang', 'approxim', 'long', 'run', 'physic', 'simul', 'someth', 'see', 'benefit', 'pleas', 'correct', 'wrong', 'thu', 'abandon', 'guest', 'demo', 'eric', 'iverson', 'itti', 'bitti', 'search', 'februari', '16th', 'daniel', 'tunkelang', 'respond', 'back', 'vacat', 'still', 'dig']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "palabras_truncadas=[]\n",
    "for word in words_filtered:\n",
    "   palabras_truncadas.append(stemmer.stem(word))\n",
    "print(palabras_truncadas[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('long', 'time', 'see', 'like', 'alway', 'rewrit', 'scratch', 'coupl', 'nevertheless', 'still')\n",
      "(1079, 4182, 2189, 4510, 1075, 8, 48, 507, 27, 1391)\n",
      "36124\n"
     ]
    }
   ],
   "source": [
    "# contando palabras con Counter\n",
    "from collections import Counter\n",
    "\n",
    "count_sin_red = Counter(palabras_truncadas)\n",
    "labels, values = zip(*count_sin_red.items())\n",
    "print(labels[:10])\n",
    "print(values[:10])\n",
    "print(len(count_sin_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, tenemos un vocabulario de 36124 palabras, procederemos a ver si hay alguna forma de reducirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 5.0, 6.0, 9.0, 14.0, 25.0, 69.0]\n"
     ]
    }
   ],
   "source": [
    "# vemos los datos en deciles\n",
    "import statistics\n",
    "deciles = statistics.quantiles(count_sin_red.values(), n=20)\n",
    "print(deciles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el análisis por cuantiles (n=20), vemos que la mayoría de las palabras tienen una aparición de entre 1 y 2 veces en el vocabulario, por lo que eliminar solamente las de 1 aparición quitaría aproximadamente el 40% del mismo lo que es demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44081497065662717"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({k: v for k, v in count_sin_red.items() if v == 1}) / len(count_sin_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser exactos, el 44%.\n",
    "\n",
    "Evitando reducir demasiado el vocabulario, se analizará el vocabulario conforme el largo de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20468386668143063"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({k: v for k, v in count_sin_red.items() if len(k) <= 4}) / len(count_sin_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un análisis rápido nos arroja que si reducimos el vocabulario con las palabras solo de largo 4 o más, lo acortaremos en solo un 20.46%, lo cual es ideal para nuestra aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We deleted 7394 words from the vocabulary\n",
      "28730\n"
     ]
    }
   ],
   "source": [
    "reduced_words = {k: v for k, v in count_sin_red.items() if len(k) > 4}\n",
    "print(\"We deleted \" + str(len(labels) - len(reduced_words)) + \" words from the vocabulary\")\n",
    "print(len(reduced_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_words = dict(count_sin_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ya tenemos un vocabulario reducido y uno sin reducir con los cuales haremos las tareas de clasificación. No sin antes realizar el modelado vectorial con los esquemas tf y tf-idf de ambos vocabularios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antes, guardaremos ambos vocabularios\n",
    "import json\n",
    "with open('resultados\\\\vocabularios\\\\voc_normal.json', 'w') as f:\n",
    "    f.write(json.dumps(original_words))\n",
    "    f.close()\n",
    "with open('resultados\\\\vocabularios\\\\voc_reducido.json', 'w') as f:\n",
    "    f.write(json.dumps(reduced_words))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Recuperamos ambos vocabularios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re, string\n",
    "with open('resultados\\\\vocabularios\\\\voc_normal.json', 'r') as file:\n",
    "    original_words = json.load(file)\n",
    "\n",
    "with open('resultados\\\\vocabularios\\\\voc_reducido.json', 'r') as file:\n",
    "    reduced_words = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blog_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documentos\\Repositorios\\Repo\\Proyecto Final\\pf.ipynb Celda 30\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documentos/Repositorios/Repo/Proyecto%20Final/pf.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#def tokenize_text(text: str, re_punc):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documentos/Repositorios/Repo/Proyecto%20Final/pf.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m docs_sep \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documentos/Repositorios/Repo/Proyecto%20Final/pf.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m blog_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documentos/Repositorios/Repo/Proyecto%20Final/pf.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     docs_sep\u001b[39m.\u001b[39mappend(tokenize_text(text\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(w), re_punc\u001b[39m=\u001b[39mre_punc))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blog_df' is not defined"
     ]
    }
   ],
   "source": [
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#def tokenize_text(text: str, re_punc):\n",
    "docs_sep = list()\n",
    "for w in blog_df['text']:\n",
    "    docs_sep.append(tokenize_text(text=str(w), re_punc=re_punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'guest',\n",
       " 'demo',\n",
       " 'eric',\n",
       " 'iverson',\n",
       " 's',\n",
       " 'itty',\n",
       " 'bitty',\n",
       " 'search',\n",
       " 'february']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sep[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0001pt': 6,\n",
       " '000rpm': 1,\n",
       " '000th': 1,\n",
       " '000vnd': 1,\n",
       " '00am': 12,\n",
       " '00i': 1,\n",
       " '00p': 4,\n",
       " '00pm': 4,\n",
       " '024x600': 1,\n",
       " '04am': 2,\n",
       " '04pm': 1,\n",
       " '0in': 23,\n",
       " '0j6': 1,\n",
       " '0mm': 1,\n",
       " '0o': 1,\n",
       " '0pt': 8,\n",
       " '0px': 14,\n",
       " '0rc1': 1,\n",
       " '0x16': 1,\n",
       " '0x1e9406': 1,\n",
       " '0x3c': 1,\n",
       " '0x44': 1,\n",
       " '0x50': 1,\n",
       " '0xa': 1,\n",
       " '0xc9': 1,\n",
       " '100': 3,\n",
       " '1000tag': 1,\n",
       " '1005pe': 3,\n",
       " '100g': 2,\n",
       " '100k': 4,\n",
       " '100kg': 1,\n",
       " '100kilo': 1,\n",
       " '100lb': 1,\n",
       " '100m': 1,\n",
       " '100mbp': 1,\n",
       " '100th': 2,\n",
       " '102cm': 1,\n",
       " '103rd': 1,\n",
       " '105degreesacademi': 1,\n",
       " '105mm': 2,\n",
       " '1080p': 1,\n",
       " '108lb': 1,\n",
       " '10am': 7,\n",
       " '10cm': 2,\n",
       " '10g': 1,\n",
       " '10k': 8,\n",
       " '10kg': 1,\n",
       " '10m': 1,\n",
       " '10million': 1,\n",
       " '10nth': 2,\n",
       " '10pm': 3,\n",
       " '10th': 22,\n",
       " '10yr': 1,\n",
       " '1100ce': 1,\n",
       " '110gm': 1,\n",
       " '112g': 1,\n",
       " '112th': 1,\n",
       " '11a': 2,\n",
       " '11am': 3,\n",
       " '11m': 1,\n",
       " '11pm': 4,\n",
       " '11st': 2,\n",
       " '11th': 19,\n",
       " '120mg': 1,\n",
       " '124c': 1,\n",
       " '124d': 1,\n",
       " '124g': 1,\n",
       " '124h': 1,\n",
       " '124th': 1,\n",
       " '128mb': 1,\n",
       " '12am': 3,\n",
       " '12cm': 1,\n",
       " '12gb': 1,\n",
       " '12k': 1,\n",
       " '12oz': 1,\n",
       " '12pm': 4,\n",
       " '12second': 2,\n",
       " '12th': 16,\n",
       " '12v': 1,\n",
       " '1300th': 1,\n",
       " '130k': 1,\n",
       " '13rm': 1,\n",
       " '13st': 8,\n",
       " '13th': 12,\n",
       " '13yr': 1,\n",
       " '14a': 1,\n",
       " '14h': 1,\n",
       " '14th': 9,\n",
       " '14visit': 1,\n",
       " '1500pt': 1,\n",
       " '150th': 2,\n",
       " '1530ce': 1,\n",
       " '15am': 1,\n",
       " '15h00': 1,\n",
       " '15million': 1,\n",
       " '15pm': 1,\n",
       " '15th': 11,\n",
       " '1600': 2,\n",
       " '160g': 1,\n",
       " '16gb': 2,\n",
       " '16m': 1,\n",
       " '16mm': 2,\n",
       " '16th': 10,\n",
       " '16x20': 1,\n",
       " '175ml': 1,\n",
       " '17th': 13,\n",
       " '1840': 1,\n",
       " '1850': 1,\n",
       " '1864dear': 1,\n",
       " '186lb': 1,\n",
       " '1870': 2,\n",
       " '1890': 2,\n",
       " '18h': 1,\n",
       " '18inch': 1,\n",
       " '18k': 1,\n",
       " '18kg': 1,\n",
       " '18th': 8,\n",
       " '1910': 1,\n",
       " '1920': 1,\n",
       " '1930': 7,\n",
       " '1940': 3,\n",
       " '1950': 7,\n",
       " '1960': 27,\n",
       " '1970': 12,\n",
       " '1980': 15,\n",
       " '1990': 11,\n",
       " '19gilr': 1,\n",
       " '19st': 8,\n",
       " '19th': 23,\n",
       " '1a': 1,\n",
       " '1am': 4,\n",
       " '1b': 5,\n",
       " '1bn': 1,\n",
       " '1c': 1,\n",
       " '1ce': 1,\n",
       " '1g': 1,\n",
       " '1gb': 4,\n",
       " '1kg': 1,\n",
       " '1l2': 2,\n",
       " '1lb': 1,\n",
       " '1li': 1,\n",
       " '1m': 3,\n",
       " '1mb': 1,\n",
       " '1mm': 1,\n",
       " '1pm': 6,\n",
       " '1prdpgn': 1,\n",
       " '1st': 55,\n",
       " '1v1': 1,\n",
       " '1yr': 1,\n",
       " '1ºc': 2,\n",
       " '20': 2,\n",
       " '200': 1,\n",
       " '2000': 3,\n",
       " '2002ish': 1,\n",
       " '20033lb': 1,\n",
       " '2009xterratrailrunningworldchampionshiphawaiimaxkingwin': 1,\n",
       " '200k': 3,\n",
       " '200mg': 3,\n",
       " '200ºc': 1,\n",
       " '20am': 1,\n",
       " '20c': 1,\n",
       " '20g': 1,\n",
       " '20k': 1,\n",
       " '20mg': 1,\n",
       " '20million': 1,\n",
       " '20minut': 1,\n",
       " '20sb': 2,\n",
       " '20sq': 1,\n",
       " '20th': 25,\n",
       " '20x200': 1,\n",
       " '210lb': 1,\n",
       " '21jan': 1,\n",
       " '21km': 1,\n",
       " '21st': 18,\n",
       " '220lb': 1,\n",
       " '225mile': 1,\n",
       " '22lb': 1,\n",
       " '22nd': 6,\n",
       " '22th': 1,\n",
       " '235ml': 1,\n",
       " '23rd': 6,\n",
       " '23th': 1,\n",
       " '23yr': 1,\n",
       " '24h': 1,\n",
       " '24th': 8,\n",
       " '24w': 1,\n",
       " '24x7': 3,\n",
       " '24year': 1,\n",
       " '256mb': 2,\n",
       " '25k': 2,\n",
       " '25ml': 1,\n",
       " '25pm': 1,\n",
       " '25px': 1,\n",
       " '25th': 4,\n",
       " '25year': 1,\n",
       " '26th': 10,\n",
       " '275lb': 8,\n",
       " '27b': 3,\n",
       " '27th': 9,\n",
       " '280x720': 1,\n",
       " '28am': 1,\n",
       " '28th': 6,\n",
       " '29june': 1,\n",
       " '29nd': 1,\n",
       " '29th': 12,\n",
       " '2am': 5,\n",
       " '2b': 2,\n",
       " '2d': 11,\n",
       " '2everi': 1,\n",
       " '2f10': 1,\n",
       " '2fjournal': 1,\n",
       " '2ft': 1,\n",
       " '2g': 2,\n",
       " '2gb': 7,\n",
       " '2gether': 2,\n",
       " '2gthr': 1,\n",
       " '2ish': 2,\n",
       " '2k': 3,\n",
       " '2l': 1,\n",
       " '2lb': 4,\n",
       " '2m': 2,\n",
       " '2nd': 45,\n",
       " '2pm': 3,\n",
       " '2px': 14,\n",
       " '2u': 1,\n",
       " '2x': 1,\n",
       " '2x2': 1,\n",
       " '2ºc': 1,\n",
       " '2πr': 1,\n",
       " '300dpi': 1,\n",
       " '300for': 1,\n",
       " '300g': 1,\n",
       " '300mile': 1,\n",
       " '300rpm': 1,\n",
       " '302a': 1,\n",
       " '30am': 15,\n",
       " '30d': 1,\n",
       " '30gm': 1,\n",
       " '30kg': 1,\n",
       " '30mg': 1,\n",
       " '30pm': 12,\n",
       " '30rockrecapgangwayforfoot': 1,\n",
       " '30th': 10,\n",
       " '31st': 8,\n",
       " '32gb': 4,\n",
       " '32mb': 1,\n",
       " '32pm': 1,\n",
       " '33ghz': 1,\n",
       " '33yr': 1,\n",
       " '340gm': 1,\n",
       " '345º': 1,\n",
       " '348º': 1,\n",
       " '34th': 3,\n",
       " '35000rupiah': 1,\n",
       " '350º': 1,\n",
       " '350ºf': 1,\n",
       " '35k': 1,\n",
       " '35mm': 3,\n",
       " '362ºc': 1,\n",
       " '365day': 1,\n",
       " '366x768': 1,\n",
       " '36m': 1,\n",
       " '36million': 1,\n",
       " '36th': 2,\n",
       " '37signal': 1,\n",
       " '380w': 1,\n",
       " '389pp': 1,\n",
       " '39a': 1,\n",
       " '39fork': 1,\n",
       " '39rm': 1,\n",
       " '39th': 1,\n",
       " '3a': 2,\n",
       " '3am': 6,\n",
       " '3b': 3,\n",
       " '3bn': 1,\n",
       " '3bt': 6,\n",
       " '3d': 33,\n",
       " '3dstripbook': 1,\n",
       " '3dtv': 1,\n",
       " '3e': 1,\n",
       " '3g': 25,\n",
       " '3k': 1,\n",
       " '3lb': 1,\n",
       " '3m': 1,\n",
       " '3mi': 1,\n",
       " '3mm154m2ndi': 8,\n",
       " '3oth': 1,\n",
       " '3pm': 5,\n",
       " '3px': 14,\n",
       " '3rd': 49,\n",
       " '3x': 2,\n",
       " '3x3': 1,\n",
       " '40': 9,\n",
       " '400k': 1,\n",
       " '400m': 2,\n",
       " '400º': 1,\n",
       " '401k': 1,\n",
       " '4050r': 1,\n",
       " '40c': 1,\n",
       " '40gb': 1,\n",
       " '40k': 5,\n",
       " '40lb': 1,\n",
       " '40mg': 1,\n",
       " '40pm': 2,\n",
       " '40th': 2,\n",
       " '42nd': 1,\n",
       " '42s02': 1,\n",
       " '436a': 1,\n",
       " '436awa': 1,\n",
       " '450w': 1,\n",
       " '45pm': 2,\n",
       " '460w': 1,\n",
       " '460xvr': 1,\n",
       " '465m': 1,\n",
       " '46bn': 1,\n",
       " '470ml': 1,\n",
       " '474ml': 1,\n",
       " '47gb': 1,\n",
       " '480x360': 1,\n",
       " '48gb': 1,\n",
       " '4ad': 3,\n",
       " '4am': 6,\n",
       " '4chan': 2,\n",
       " '4cm': 2,\n",
       " '4everfrom': 1,\n",
       " '4evr': 1,\n",
       " '4f': 1,\n",
       " '4g': 3,\n",
       " '4gb': 1,\n",
       " '4get': 2,\n",
       " '4gt': 2,\n",
       " '4h': 2,\n",
       " '4h30': 1,\n",
       " '4k': 1,\n",
       " '4lb': 12,\n",
       " '4m': 2,\n",
       " '4pm': 4,\n",
       " '4pt': 14,\n",
       " '4rm': 1,\n",
       " '4rom': 1,\n",
       " '4sitwc': 2,\n",
       " '4sq': 4,\n",
       " '4squar': 1,\n",
       " '4th': 31,\n",
       " '4trn': 1,\n",
       " '4wd': 1,\n",
       " '4x': 1,\n",
       " '4x4': 1,\n",
       " '4yr': 2,\n",
       " '50': 7,\n",
       " '5001a': 1,\n",
       " '500g': 1,\n",
       " '500gb': 1,\n",
       " '500lb': 1,\n",
       " '500ml': 1,\n",
       " '500with': 1,\n",
       " '50cm': 3,\n",
       " '50e': 2,\n",
       " '50gb': 1,\n",
       " '50gm': 2,\n",
       " '50k': 12,\n",
       " '50km': 1,\n",
       " '50lb': 1,\n",
       " '50mb': 1,\n",
       " '50mg': 1,\n",
       " '50th': 4,\n",
       " '50w': 1,\n",
       " '50x50': 1,\n",
       " '50x60': 1,\n",
       " '50x66': 1,\n",
       " '512k': 1,\n",
       " '512mb': 1,\n",
       " '51st': 1,\n",
       " '5200r': 1,\n",
       " '55gm': 2,\n",
       " '55mm': 1,\n",
       " '55pm': 1,\n",
       " '55th': 1,\n",
       " '573pp': 1,\n",
       " '59p': 1,\n",
       " '59pm': 1,\n",
       " '5am': 4,\n",
       " '5bn': 1,\n",
       " '5ft': 1,\n",
       " '5g': 1,\n",
       " '5gb': 1,\n",
       " '5k': 8,\n",
       " '5km': 1,\n",
       " '5mb': 1,\n",
       " '5mg': 3,\n",
       " '5ml': 3,\n",
       " '5pm': 3,\n",
       " '5th': 26,\n",
       " '5x': 1,\n",
       " '5x7': 1,\n",
       " '5º': 1,\n",
       " '60': 8,\n",
       " '600mg': 1,\n",
       " '60f': 1,\n",
       " '60k': 3,\n",
       " '60ml': 1,\n",
       " '60x60': 1,\n",
       " '62nd': 2,\n",
       " '640x480': 1,\n",
       " '64bit': 1,\n",
       " '64gb': 2,\n",
       " '64º': 1,\n",
       " '65k': 1,\n",
       " '65kg': 1,\n",
       " '65lb': 1,\n",
       " '65m': 1,\n",
       " '65th': 2,\n",
       " '6600r': 1,\n",
       " '66th': 3,\n",
       " '6am': 3,\n",
       " '6b': 1,\n",
       " '6gb': 2,\n",
       " '6hr': 1,\n",
       " '6lb': 5,\n",
       " '6mhz': 1,\n",
       " '6nfqa': 1,\n",
       " '6o': 1,\n",
       " '6pm': 7,\n",
       " '6s': 1,\n",
       " '6th': 21,\n",
       " '6x6': 1,\n",
       " '6yr': 1,\n",
       " '70': 8,\n",
       " '7025jeev': 1,\n",
       " '720p': 2,\n",
       " '72dpi': 1,\n",
       " '72ppi': 1,\n",
       " '730am': 1,\n",
       " '75': 1,\n",
       " '75gm': 2,\n",
       " '75mg': 1,\n",
       " '75th': 2,\n",
       " '77wabc': 1,\n",
       " '785g': 1,\n",
       " '785gmt': 1,\n",
       " '7am': 2,\n",
       " '7e': 1,\n",
       " '7gb': 1,\n",
       " '7kg': 1,\n",
       " '7km': 1,\n",
       " '7lb': 1,\n",
       " '7m': 3,\n",
       " '7m7ezmsthdibhz5bzvynd': 3,\n",
       " '7pm': 13,\n",
       " '7th': 21,\n",
       " '7x7': 3,\n",
       " '80': 28,\n",
       " '800ºc': 1,\n",
       " '80a': 2,\n",
       " '80g': 1,\n",
       " '81mg': 1,\n",
       " '82nd': 2,\n",
       " '83rd': 1,\n",
       " '84th': 1,\n",
       " '87th': 1,\n",
       " '88cm': 1,\n",
       " '89lb': 1,\n",
       " '89th': 1,\n",
       " '8am': 5,\n",
       " '8bn': 1,\n",
       " '8c': 1,\n",
       " '8em': 14,\n",
       " '8gb': 2,\n",
       " '8gm': 1,\n",
       " '8h': 1,\n",
       " '8k': 1,\n",
       " '8kg': 3,\n",
       " '8lb': 8,\n",
       " '8mm': 1,\n",
       " '8pm': 10,\n",
       " '8th': 20,\n",
       " '8x10': 3,\n",
       " '8xbk9q': 1,\n",
       " '90': 16,\n",
       " '900k': 1,\n",
       " '906266p2': 1,\n",
       " '90mg': 1,\n",
       " '90of': 1,\n",
       " '92nd': 1,\n",
       " '94th': 1,\n",
       " '95gm': 1,\n",
       " '969м': 1,\n",
       " '980x': 1,\n",
       " '999th': 1,\n",
       " '9akqia': 1,\n",
       " '9am': 8,\n",
       " '9b': 3,\n",
       " '9c': 2,\n",
       " '9ish': 1,\n",
       " '9lb': 9,\n",
       " '9oz': 1,\n",
       " '9pm': 11,\n",
       " '9pmnj9': 1,\n",
       " '9tbjlg': 1,\n",
       " '9th': 9,\n",
       " '9wiqyv': 1,\n",
       " '9xxrsz': 2,\n",
       " '9yr': 1,\n",
       " 'a2tay9': 1,\n",
       " 'a3': 1,\n",
       " 'a300': 1,\n",
       " 'a4': 3,\n",
       " 'a7x': 2,\n",
       " 'a874': 1,\n",
       " 'aa': 8,\n",
       " 'aaa': 2,\n",
       " 'aaaa': 1,\n",
       " 'aaaaaaaaa': 1,\n",
       " 'aaaaah': 1,\n",
       " 'aaaaand': 1,\n",
       " 'aaaah': 2,\n",
       " 'aaaand': 1,\n",
       " 'aaah': 2,\n",
       " 'aaahhaaa': 1,\n",
       " 'aacross': 1,\n",
       " 'aada': 1,\n",
       " 'aadi': 2,\n",
       " 'aaduvaa': 1,\n",
       " 'aadyqf': 1,\n",
       " 'aafp': 15,\n",
       " 'aah': 1,\n",
       " 'aaj': 1,\n",
       " 'aam': 2,\n",
       " 'aamir': 4,\n",
       " 'aana': 1,\n",
       " 'aandolan': 1,\n",
       " 'aap': 2,\n",
       " 'aarathi': 1,\n",
       " 'aaron': 10,\n",
       " 'aarp': 4,\n",
       " 'aarthi': 3,\n",
       " 'aarush': 1,\n",
       " 'aasai': 1,\n",
       " 'aasif': 1,\n",
       " 'aaspir': 1,\n",
       " 'aata': 3,\n",
       " 'aathu': 1,\n",
       " 'aawvmq': 1,\n",
       " 'aayega': 1,\n",
       " 'ab': 11,\n",
       " 'aba': 2,\n",
       " 'ababa': 1,\n",
       " 'aback': 5,\n",
       " 'abacu': 1,\n",
       " 'abad': 1,\n",
       " 'abaixo': 3,\n",
       " 'abala': 1,\n",
       " 'abandon': 40,\n",
       " 'abandonada': 2,\n",
       " 'abat': 2,\n",
       " 'abb': 1,\n",
       " 'abba': 1,\n",
       " 'abbess': 1,\n",
       " 'abbey': 5,\n",
       " 'abbi': 6,\n",
       " 'abbott': 2,\n",
       " 'abbrevi': 1,\n",
       " 'abc': 20,\n",
       " 'abc800': 1,\n",
       " 'abcd': 8,\n",
       " 'abcedarian': 1,\n",
       " 'abcloc': 1,\n",
       " 'abdalhaleem': 1,\n",
       " 'abdallat': 1,\n",
       " 'abdel': 9,\n",
       " 'abdelwahid': 1,\n",
       " 'abdi': 1,\n",
       " 'abdomen': 3,\n",
       " 'abdomin': 7,\n",
       " 'abdominoplasti': 4,\n",
       " 'abduct': 4,\n",
       " 'abdul': 11,\n",
       " 'abdulmutallab': 5,\n",
       " 'abdulwahid': 11,\n",
       " 'abe': 1,\n",
       " 'abeauti': 1,\n",
       " 'abednego': 2,\n",
       " 'abel': 4,\n",
       " 'abelard': 14,\n",
       " 'abercrombi': 2,\n",
       " 'aberdeen': 1,\n",
       " 'abernathi': 1,\n",
       " 'abet': 1,\n",
       " 'abhik': 1,\n",
       " 'abhinav': 3,\n",
       " 'abhishekontheweb': 1,\n",
       " 'abhor': 3,\n",
       " 'abhyu': 3,\n",
       " 'abhyuday': 2,\n",
       " 'abid': 8,\n",
       " 'abigail': 1,\n",
       " 'abil': 153,\n",
       " 'abilifi': 3,\n",
       " 'abirami': 1,\n",
       " 'abject': 2,\n",
       " 'abl': 512,\n",
       " 'ablaz': 1,\n",
       " 'abli': 5,\n",
       " 'ablum': 2,\n",
       " 'abn': 1,\n",
       " 'abnorm': 6,\n",
       " 'abnorrm': 1,\n",
       " 'aboard': 6,\n",
       " 'abod': 2,\n",
       " 'aboiut': 1,\n",
       " 'aboleth': 1,\n",
       " 'abolish': 1,\n",
       " 'abolit': 5,\n",
       " 'abolitionist': 1,\n",
       " 'abomin': 3,\n",
       " 'aborigin': 11,\n",
       " 'abort': 25,\n",
       " 'abou': 2,\n",
       " 'abound': 7,\n",
       " 'abouyt': 1,\n",
       " 'abra': 1,\n",
       " 'abracadabra': 1,\n",
       " 'abraham': 9,\n",
       " 'abram': 1,\n",
       " 'abranch': 2,\n",
       " 'abras': 2,\n",
       " 'abreast': 1,\n",
       " 'abri': 2,\n",
       " 'abridg': 1,\n",
       " 'abriu': 1,\n",
       " 'abroad': 28,\n",
       " 'abrupt': 1,\n",
       " 'abruptli': 4,\n",
       " 'abscenc': 1,\n",
       " 'abscess': 7,\n",
       " 'abscond': 2,\n",
       " 'abseil': 1,\n",
       " 'absenc': 29,\n",
       " 'absens': 1,\n",
       " 'absent': 16,\n",
       " 'absente': 4,\n",
       " 'absinth': 1,\n",
       " 'absolu': 1,\n",
       " 'absolut': 178,\n",
       " 'absolutli': 2,\n",
       " 'absorb': 28,\n",
       " 'abstain': 1,\n",
       " 'abstent': 1,\n",
       " 'abstin': 2,\n",
       " 'abstract': 21,\n",
       " 'abstractli': 2,\n",
       " 'abstruct': 1,\n",
       " 'absurd': 22,\n",
       " 'absurdli': 4,\n",
       " 'absurdum': 1,\n",
       " 'abt': 108,\n",
       " 'abu': 9,\n",
       " 'abuja': 2,\n",
       " 'abukhalil': 1,\n",
       " 'abulm': 1,\n",
       " 'abund': 13,\n",
       " 'abundantli': 1,\n",
       " 'abus': 67,\n",
       " 'abut': 3,\n",
       " 'abuzz': 1,\n",
       " 'abysm': 3,\n",
       " 'abyss': 6,\n",
       " 'abyssinia': 1,\n",
       " 'ac': 8,\n",
       " 'ac360': 1,\n",
       " 'acaba': 1,\n",
       " 'acabara': 1,\n",
       " 'academ': 56,\n",
       " 'academi': 30,\n",
       " 'academia': 13,\n",
       " 'academo': 1,\n",
       " 'acai': 3,\n",
       " 'acap': 1,\n",
       " 'acapulco': 1,\n",
       " 'acc': 1,\n",
       " 'accel': 1,\n",
       " 'acceler': 23,\n",
       " 'accent': 27,\n",
       " 'accentu': 3,\n",
       " 'accept': 189,\n",
       " 'acceso': 1,\n",
       " 'access': 177,\n",
       " 'accesscontrol': 1,\n",
       " 'accessoir': 1,\n",
       " 'accessor': 2,\n",
       " 'accessori': 31,\n",
       " 'accessoris': 2,\n",
       " 'accid': 51,\n",
       " 'accident': 19,\n",
       " 'acclaim': 12,\n",
       " 'acclim': 1,\n",
       " 'acclimatis': 1,\n",
       " 'accolad': 4,\n",
       " 'accommod': 47,\n",
       " 'accomod': 3,\n",
       " 'accompagn': 1,\n",
       " 'accompani': 43,\n",
       " 'accompic': 1,\n",
       " 'accomplish': 57,\n",
       " 'accoplish': 1,\n",
       " 'accord': 167,\n",
       " 'accordin': 1,\n",
       " 'accordingli': 13,\n",
       " 'account': 157,\n",
       " 'accountdata': 5,\n",
       " 'accoutr': 3,\n",
       " 'accpac': 1,\n",
       " 'accra': 1,\n",
       " 'accret': 1,\n",
       " 'accross': 1,\n",
       " 'accru': 3,\n",
       " 'acctual': 1,\n",
       " 'accumul': 13,\n",
       " 'accur': 50,\n",
       " 'accuraci': 13,\n",
       " 'accus': 93,\n",
       " 'accusatori': 1,\n",
       " 'accustom': 11,\n",
       " 'accuweath': 1,\n",
       " 'ace': 12,\n",
       " 'aceitarei': 1,\n",
       " 'aceito': 1,\n",
       " 'aceo': 2,\n",
       " 'acept': 1,\n",
       " 'acessori': 1,\n",
       " 'acetaminophen': 2,\n",
       " 'acetamophin': 1,\n",
       " 'ach': 27,\n",
       " 'achaar': 1,\n",
       " 'achalasia': 6,\n",
       " 'achalsia': 1,\n",
       " 'acheiv': 1,\n",
       " 'achiev': 138,\n",
       " 'achil': 1,\n",
       " 'achincalhado': 1,\n",
       " 'achingli': 1,\n",
       " 'achu': 8,\n",
       " 'achudhanandan': 1,\n",
       " 'acid': 32,\n",
       " 'acit': 1,\n",
       " 'ack': 5,\n",
       " 'ackatsi': 1,\n",
       " 'acker': 2,\n",
       " 'acknowledg': 36,\n",
       " 'acl': 6,\n",
       " 'aclu': 2,\n",
       " 'acm': 1,\n",
       " 'acn': 10,\n",
       " 'acolyt': 2,\n",
       " 'acompagn': 1,\n",
       " 'acontec': 3,\n",
       " 'acontecem': 1,\n",
       " 'aconteceu': 2,\n",
       " 'acontecimento': 2,\n",
       " 'acorn': 5,\n",
       " 'acoust': 12,\n",
       " 'acp': 2,\n",
       " 'acquaint': 11,\n",
       " 'acquaintanceship': 1,\n",
       " 'acquiesc': 2,\n",
       " 'acquir': 33,\n",
       " 'acquisit': 11,\n",
       " 'acquit': 1,\n",
       " 'acr': 16,\n",
       " 'acredito': 1,\n",
       " 'acrimoni': 1,\n",
       " 'acrooss': 1,\n",
       " 'across': 328,\n",
       " 'acrossth': 1,\n",
       " 'acrost': 1,\n",
       " 'acryl': 18,\n",
       " 'act': 270,\n",
       " 'acta': 6,\n",
       " 'action': 260,\n",
       " 'actionaround': 1,\n",
       " 'actionscript': 1,\n",
       " 'activ': 221,\n",
       " 'activis': 2,\n",
       " 'activist': 19,\n",
       " 'activit': 1,\n",
       " 'activli': 1,\n",
       " 'acton': 2,\n",
       " 'actor': 79,\n",
       " 'actress': 38,\n",
       " 'actressi': 1,\n",
       " 'actsasvers': 3,\n",
       " 'actual': 802,\n",
       " 'actuali': 1,\n",
       " 'actuat': 2,\n",
       " 'acupunctur': 3,\n",
       " 'acura': 7,\n",
       " 'acut': 9,\n",
       " 'acuvu': 1,\n",
       " 'acá': 8,\n",
       " 'ad': 406,\n",
       " 'ada': 5,\n",
       " 'adag': 3,\n",
       " 'adak': 2,\n",
       " 'adakah': 1,\n",
       " 'adakamaana': 1,\n",
       " 'adala': 1,\n",
       " 'adalah': 2,\n",
       " 'adam': 40,\n",
       " 'adamantli': 2,\n",
       " 'adambakkam': 1,\n",
       " 'adap': 2,\n",
       " 'adapt': 64,\n",
       " 'adaverallam': 1,\n",
       " 'aday': 1,\n",
       " 'adblock': 2,\n",
       " 'add': 386,\n",
       " 'addam': 8,\n",
       " 'addenbrook': 1,\n",
       " 'addendum': 1,\n",
       " 'adderal': 2,\n",
       " 'addi': 2,\n",
       " 'addict': 62,\n",
       " 'addit': 172,\n",
       " 'additon': 1,\n",
       " 'additud': 1,\n",
       " 'addl': 1,\n",
       " 'addon': 2,\n",
       " 'address': 120,\n",
       " 'addressbar': 2,\n",
       " 'addresse': 1,\n",
       " 'addtion': 1,\n",
       " 'addustour': 1,\n",
       " 'addybel': 1,\n",
       " 'addycat': 1,\n",
       " 'addyson': 3,\n",
       " 'ade': 1,\n",
       " 'adebayor': 2,\n",
       " 'adej': 1,\n",
       " 'adel': 2,\n",
       " 'adela': 2,\n",
       " 'adelaid': 7,\n",
       " 'adelaidian': 1,\n",
       " 'adelson': 6,\n",
       " 'adelstein': 2,\n",
       " 'aden': 2,\n",
       " 'adenoid': 1,\n",
       " 'adenoma': 1,\n",
       " 'adenovirida': 1,\n",
       " 'adenoviru': 1,\n",
       " 'adept': 9,\n",
       " 'adequ': 20,\n",
       " 'adf': 1,\n",
       " 'adgroup': 4,\n",
       " 'adha': 2,\n",
       " 'adhd': 69,\n",
       " 'adhdan': 1,\n",
       " 'adher': 11,\n",
       " 'adhes': 3,\n",
       " 'adhirade': 1,\n",
       " 'adhu': 1,\n",
       " 'adhukellam': 1,\n",
       " 'adhunik': 2,\n",
       " 'adhérent': 1,\n",
       " 'adi': 4,\n",
       " 'adicha': 1,\n",
       " 'adichaan': 1,\n",
       " 'adida': 1,\n",
       " 'adieu': 1,\n",
       " 'adikara': 1,\n",
       " 'adil': 2,\n",
       " 'adina': 1,\n",
       " 'adio': 2,\n",
       " 'adippa': 1,\n",
       " 'adirondack': 2,\n",
       " 'adithya': 1,\n",
       " 'aditud': 1,\n",
       " 'adjac': 12,\n",
       " 'adjcent': 1,\n",
       " 'adject': 4,\n",
       " 'adjoin': 3,\n",
       " 'adjourn': 1,\n",
       " 'adjud': 2,\n",
       " 'adjust': 64,\n",
       " 'admi': 3,\n",
       " 'admin': 4,\n",
       " 'administ': 7,\n",
       " 'administr': 71,\n",
       " 'admir': 79,\n",
       " 'admiss': 15,\n",
       " 'admist': 2,\n",
       " 'admit': 135,\n",
       " 'admitt': 1,\n",
       " 'admittedli': 11,\n",
       " 'admob': 1,\n",
       " 'admonit': 2,\n",
       " 'admor': 1,\n",
       " 'adn': 1,\n",
       " 'ado': 9,\n",
       " 'adob': 6,\n",
       " 'adobo': 1,\n",
       " 'adolesc': 4,\n",
       " 'adolf': 1,\n",
       " 'adolphu': 1,\n",
       " 'adopt': 60,\n",
       " 'ador': 60,\n",
       " 'adora': 1,\n",
       " 'adoraal': 1,\n",
       " 'adorablest': 1,\n",
       " 'adorin': 1,\n",
       " 'adorn': 11,\n",
       " 'adoré': 2,\n",
       " 'adoxograph': 2,\n",
       " 'adra': 1,\n",
       " 'adren': 1,\n",
       " 'adrenalin': 8,\n",
       " 'adrian': 33,\n",
       " 'adriana': 1,\n",
       " 'adriat': 1,\n",
       " 'adrienn': 1,\n",
       " 'adrift': 2,\n",
       " 'adroitli': 1,\n",
       " 'adsens': 6,\n",
       " 'adsl': 1,\n",
       " 'adt': 1,\n",
       " 'adu': 1,\n",
       " 'aducharm': 1,\n",
       " 'aduchifyin': 1,\n",
       " 'adult': 94,\n",
       " 'adultaraisin': 1,\n",
       " 'adulteri': 4,\n",
       " 'adulthood': 5,\n",
       " 'adurai': 1,\n",
       " 'advair': 2,\n",
       " 'advanc': 100,\n",
       " 'advani': 1,\n",
       " 'advantag': 83,\n",
       " 'advent': 4,\n",
       " 'adventur': 132,\n",
       " 'adventureland': 2,\n",
       " 'adverb': 1,\n",
       " 'advers': 6,\n",
       " 'adversari': 2,\n",
       " 'advert': 2,\n",
       " 'advertis': 108,\n",
       " 'advic': 124,\n",
       " 'advis': 54,\n",
       " 'advise': 1,\n",
       " 'advisor': 10,\n",
       " 'advisori': 5,\n",
       " 'advoc': 24,\n",
       " 'advocaci': 10,\n",
       " 'adwar': 2,\n",
       " 'adword': 3,\n",
       " 'adyar': 2,\n",
       " 'ae': 2,\n",
       " 'aea': 2,\n",
       " 'aer9': 6,\n",
       " 'aerat': 1,\n",
       " 'aeri': 1,\n",
       " 'aerial': 3,\n",
       " 'aero': 2,\n",
       " 'aerob': 7,\n",
       " 'aeroballoon': 1,\n",
       " 'aerodynam': 1,\n",
       " 'aeroplan': 1,\n",
       " 'aerosmith': 6,\n",
       " 'aeschylu': 1,\n",
       " 'aesthet': 15,\n",
       " 'aesthtic': 1,\n",
       " 'aether': 1,\n",
       " 'aethest': 1,\n",
       " 'afa': 2,\n",
       " 'afaik': 2,\n",
       " 'afar': 8,\n",
       " 'afarensi': 2,\n",
       " 'afb': 1,\n",
       " 'afew': 1,\n",
       " 'affabl': 1,\n",
       " 'affair': 52,\n",
       " 'affect': 120,\n",
       " 'affection': 4,\n",
       " 'affem': 1,\n",
       " 'affettati': 1,\n",
       " 'affidavit': 4,\n",
       " 'affili': 16,\n",
       " 'affin': 5,\n",
       " 'affirm': 14,\n",
       " 'affix': 3,\n",
       " 'affleck': 3,\n",
       " 'afflict': 7,\n",
       " 'affluent': 5,\n",
       " 'afford': 80,\n",
       " 'affort': 1,\n",
       " 'affront': 1,\n",
       " 'afghan': 3,\n",
       " 'afghani': 1,\n",
       " 'afghanistan': 10,\n",
       " 'aficionado': 1,\n",
       " 'afield': 1,\n",
       " 'afikomen': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_og_words = dict(sorted(original_words.items()))\n",
    "order_og_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guest',\n",
       " 'demo',\n",
       " 'eric',\n",
       " 'iverson',\n",
       " 's',\n",
       " 'itty',\n",
       " 'bitty',\n",
       " 'search',\n",
       " 'february',\n",
       " '16th',\n",
       " '2010',\n",
       " 'by',\n",
       " 'daniel',\n",
       " 'tunkelang',\n",
       " 'respond',\n",
       " 'i',\n",
       " 'm',\n",
       " 'back',\n",
       " 'from',\n",
       " 'vacation',\n",
       " 'and',\n",
       " 'still',\n",
       " 'digging',\n",
       " 'my',\n",
       " 'way',\n",
       " 'out',\n",
       " 'of',\n",
       " 'everything',\n",
       " 'that',\n",
       " 's',\n",
       " 'piled',\n",
       " 'up',\n",
       " 'while',\n",
       " 'i',\n",
       " 've',\n",
       " 'been',\n",
       " 'offline',\n",
       " 'while',\n",
       " 'i',\n",
       " 'catch',\n",
       " 'up',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'i',\n",
       " 'd',\n",
       " 'share',\n",
       " 'with',\n",
       " 'you',\n",
       " 'a',\n",
       " 'demo',\n",
       " 'that',\n",
       " 'eric',\n",
       " 'iverson',\n",
       " 'was',\n",
       " 'gracious',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'share',\n",
       " 'with',\n",
       " 'me',\n",
       " 'it',\n",
       " 'uses',\n",
       " 'yahoo',\n",
       " 'boss',\n",
       " 'to',\n",
       " 'support',\n",
       " 'an',\n",
       " 'exploratory',\n",
       " 'search',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'a',\n",
       " 'general',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'when',\n",
       " 'you',\n",
       " 'perform',\n",
       " 'a',\n",
       " 'query',\n",
       " 'the',\n",
       " 'application',\n",
       " 'retrieves',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'related',\n",
       " 'term',\n",
       " 'candidates',\n",
       " 'using',\n",
       " 'yahoo',\n",
       " 's',\n",
       " 'key',\n",
       " 'terms',\n",
       " 'api',\n",
       " 'it',\n",
       " 'then',\n",
       " 'scores',\n",
       " 'each',\n",
       " 'term',\n",
       " 'by',\n",
       " 'dividing',\n",
       " 'its',\n",
       " 'occurrence',\n",
       " 'count',\n",
       " 'within',\n",
       " 'the',\n",
       " 'result',\n",
       " 'set',\n",
       " 'by',\n",
       " 'its',\n",
       " 'global',\n",
       " 'occurrence',\n",
       " 'count',\n",
       " 'a',\n",
       " 'relevance',\n",
       " 'measure',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'one',\n",
       " 'my',\n",
       " 'former',\n",
       " 'colleagues',\n",
       " 'and',\n",
       " 'i',\n",
       " 'used',\n",
       " 'at',\n",
       " 'endeca',\n",
       " 'in',\n",
       " 'enterprise',\n",
       " 'contexts',\n",
       " 'you',\n",
       " 'can',\n",
       " 'try',\n",
       " 'out',\n",
       " 'the',\n",
       " 'demo',\n",
       " 'yourself',\n",
       " 'at',\n",
       " 'http',\n",
       " 'www',\n",
       " 'ittybittysearch',\n",
       " 'com',\n",
       " 'while',\n",
       " 'it',\n",
       " 'has',\n",
       " 'rough',\n",
       " 'edges',\n",
       " 'it',\n",
       " 'produces',\n",
       " 'nice',\n",
       " 'results',\n",
       " 'especially',\n",
       " 'considering',\n",
       " 'the',\n",
       " 'simplicity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approach',\n",
       " 'here',\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'how',\n",
       " 'i',\n",
       " 'used',\n",
       " 'the',\n",
       " 'application',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'something',\n",
       " 'new',\n",
       " 'i',\n",
       " 'started',\n",
       " 'with',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'i',\n",
       " 'noticed',\n",
       " 'interactive',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'as',\n",
       " 'a',\n",
       " 'top',\n",
       " 'term',\n",
       " 'so',\n",
       " 'i',\n",
       " 'used',\n",
       " 'it',\n",
       " 'to',\n",
       " 'refine',\n",
       " 'most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'refinement',\n",
       " 'suggestions',\n",
       " 'looked',\n",
       " 'familiar',\n",
       " 'to',\n",
       " 'me',\n",
       " 'but',\n",
       " 'an',\n",
       " 'unfamiliar',\n",
       " 'name',\n",
       " 'caught',\n",
       " 'my',\n",
       " 'attention',\n",
       " 'anton',\n",
       " 'leuski',\n",
       " 'following',\n",
       " 'my',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'refined',\n",
       " 'again',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'the',\n",
       " 'results',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'saw',\n",
       " 'that',\n",
       " 'leuski',\n",
       " 'had',\n",
       " 'done',\n",
       " 'work',\n",
       " 'on',\n",
       " 'evaluating',\n",
       " 'document',\n",
       " 'clustering',\n",
       " 'for',\n",
       " 'interactive',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'further',\n",
       " 'exploration',\n",
       " 'made',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'this',\n",
       " 'is',\n",
       " 'someone',\n",
       " 'whose',\n",
       " 'work',\n",
       " 'i',\n",
       " 'should',\n",
       " 'get',\n",
       " 'to',\n",
       " 'know',\n",
       " 'check',\n",
       " 'out',\n",
       " 'his',\n",
       " 'home',\n",
       " 'page',\n",
       " 'i',\n",
       " 'can',\n",
       " 't',\n",
       " 'promise',\n",
       " 'that',\n",
       " 'you',\n",
       " 'll',\n",
       " 'have',\n",
       " 'as',\n",
       " 'productive',\n",
       " 'an',\n",
       " 'experience',\n",
       " 'as',\n",
       " 'i',\n",
       " 'did',\n",
       " 'but',\n",
       " 'i',\n",
       " 'encourage',\n",
       " 'you',\n",
       " 'to',\n",
       " 'try',\n",
       " 'eric',\n",
       " 's',\n",
       " 'demo',\n",
       " 'it',\n",
       " 's',\n",
       " 'simple',\n",
       " 'examples',\n",
       " 'like',\n",
       " 'these',\n",
       " 'that',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'pursuing',\n",
       " 'hcir',\n",
       " 'for',\n",
       " 'the',\n",
       " 'open',\n",
       " 'web',\n",
       " 'speaking',\n",
       " 'of',\n",
       " 'which',\n",
       " 'hcir',\n",
       " '2010',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'works',\n",
       " 'we',\n",
       " 'll',\n",
       " 'flesh',\n",
       " 'out',\n",
       " 'the',\n",
       " 'details',\n",
       " 'over',\n",
       " 'the',\n",
       " 'next',\n",
       " 'weeks',\n",
       " 'and',\n",
       " 'of',\n",
       " 'course',\n",
       " 'i',\n",
       " 'll',\n",
       " 'share',\n",
       " 'them',\n",
       " 'here']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"contamos por documento\"\"\" \n",
    "## vocabulario completo ##\n",
    "# blog_df\n",
    "from operator import countOf\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from operator import countOf\n",
    "\n",
    "\n",
    "doc_words_per_doc = dict()\n",
    "i = 1\n",
    "for w in blog_df['text']:\n",
    "    doc_words_per_doc[i] = [x for x in tokenize_text(text=str(w), re_punc=re_punc) if x]\n",
    "    i= i+1\n",
    "doc_words_per_doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_doc_completo = dict()\n",
    "for d in doc_words_per_doc.keys():\n",
    "    freq_per_doc_completo[d] = dict().fromkeys(order_og_words.keys())\n",
    "    for t in order_og_words.keys():\n",
    "        freq_per_doc_completo[d][t] = countOf(doc_words_per_doc[d], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0001pt': 0,\n",
       " '000rpm': 0,\n",
       " '000th': 0,\n",
       " '000vnd': 0,\n",
       " '00am': 0,\n",
       " '00i': 0,\n",
       " '00p': 0,\n",
       " '00pm': 0,\n",
       " '024x600': 0,\n",
       " '04am': 0,\n",
       " '04pm': 0,\n",
       " '0in': 0,\n",
       " '0j6': 0,\n",
       " '0mm': 0,\n",
       " '0o': 0,\n",
       " '0pt': 0,\n",
       " '0px': 0,\n",
       " '0rc1': 0,\n",
       " '0x16': 0,\n",
       " '0x1e9406': 0,\n",
       " '0x3c': 0,\n",
       " '0x44': 0,\n",
       " '0x50': 0,\n",
       " '0xa': 0,\n",
       " '0xc9': 0,\n",
       " '100': 0,\n",
       " '1000tag': 0,\n",
       " '1005pe': 0,\n",
       " '100g': 0,\n",
       " '100k': 0,\n",
       " '100kg': 0,\n",
       " '100kilo': 0,\n",
       " '100lb': 0,\n",
       " '100m': 0,\n",
       " '100mbp': 0,\n",
       " '100th': 0,\n",
       " '102cm': 0,\n",
       " '103rd': 0,\n",
       " '105degreesacademi': 0,\n",
       " '105mm': 0,\n",
       " '1080p': 0,\n",
       " '108lb': 0,\n",
       " '10am': 0,\n",
       " '10cm': 0,\n",
       " '10g': 0,\n",
       " '10k': 0,\n",
       " '10kg': 0,\n",
       " '10m': 0,\n",
       " '10million': 0,\n",
       " '10nth': 0,\n",
       " '10pm': 0,\n",
       " '10th': 0,\n",
       " '10yr': 0,\n",
       " '1100ce': 0,\n",
       " '110gm': 0,\n",
       " '112g': 0,\n",
       " '112th': 0,\n",
       " '11a': 0,\n",
       " '11am': 0,\n",
       " '11m': 0,\n",
       " '11pm': 0,\n",
       " '11st': 0,\n",
       " '11th': 0,\n",
       " '120mg': 0,\n",
       " '124c': 0,\n",
       " '124d': 0,\n",
       " '124g': 0,\n",
       " '124h': 0,\n",
       " '124th': 0,\n",
       " '128mb': 0,\n",
       " '12am': 0,\n",
       " '12cm': 0,\n",
       " '12gb': 0,\n",
       " '12k': 0,\n",
       " '12oz': 0,\n",
       " '12pm': 0,\n",
       " '12second': 0,\n",
       " '12th': 0,\n",
       " '12v': 0,\n",
       " '1300th': 0,\n",
       " '130k': 0,\n",
       " '13rm': 0,\n",
       " '13st': 0,\n",
       " '13th': 0,\n",
       " '13yr': 0,\n",
       " '14a': 0,\n",
       " '14h': 0,\n",
       " '14th': 0,\n",
       " '14visit': 0,\n",
       " '1500pt': 0,\n",
       " '150th': 0,\n",
       " '1530ce': 0,\n",
       " '15am': 0,\n",
       " '15h00': 0,\n",
       " '15million': 0,\n",
       " '15pm': 0,\n",
       " '15th': 0,\n",
       " '1600': 0,\n",
       " '160g': 0,\n",
       " '16gb': 0,\n",
       " '16m': 0,\n",
       " '16mm': 0,\n",
       " '16th': 0,\n",
       " '16x20': 0,\n",
       " '175ml': 0,\n",
       " '17th': 0,\n",
       " '1840': 0,\n",
       " '1850': 0,\n",
       " '1864dear': 0,\n",
       " '186lb': 0,\n",
       " '1870': 0,\n",
       " '1890': 0,\n",
       " '18h': 0,\n",
       " '18inch': 0,\n",
       " '18k': 0,\n",
       " '18kg': 0,\n",
       " '18th': 0,\n",
       " '1910': 0,\n",
       " '1920': 0,\n",
       " '1930': 0,\n",
       " '1940': 0,\n",
       " '1950': 0,\n",
       " '1960': 0,\n",
       " '1970': 0,\n",
       " '1980': 0,\n",
       " '1990': 0,\n",
       " '19gilr': 0,\n",
       " '19st': 0,\n",
       " '19th': 0,\n",
       " '1a': 0,\n",
       " '1am': 0,\n",
       " '1b': 0,\n",
       " '1bn': 0,\n",
       " '1c': 0,\n",
       " '1ce': 0,\n",
       " '1g': 0,\n",
       " '1gb': 0,\n",
       " '1kg': 0,\n",
       " '1l2': 0,\n",
       " '1lb': 0,\n",
       " '1li': 0,\n",
       " '1m': 0,\n",
       " '1mb': 0,\n",
       " '1mm': 0,\n",
       " '1pm': 0,\n",
       " '1prdpgn': 0,\n",
       " '1st': 0,\n",
       " '1v1': 0,\n",
       " '1yr': 0,\n",
       " '1ºc': 0,\n",
       " '20': 0,\n",
       " '200': 0,\n",
       " '2000': 0,\n",
       " '2002ish': 0,\n",
       " '20033lb': 0,\n",
       " '2009xterratrailrunningworldchampionshiphawaiimaxkingwin': 0,\n",
       " '200k': 0,\n",
       " '200mg': 0,\n",
       " '200ºc': 0,\n",
       " '20am': 0,\n",
       " '20c': 0,\n",
       " '20g': 0,\n",
       " '20k': 0,\n",
       " '20mg': 0,\n",
       " '20million': 0,\n",
       " '20minut': 0,\n",
       " '20sb': 0,\n",
       " '20sq': 0,\n",
       " '20th': 0,\n",
       " '20x200': 0,\n",
       " '210lb': 0,\n",
       " '21jan': 0,\n",
       " '21km': 0,\n",
       " '21st': 0,\n",
       " '220lb': 0,\n",
       " '225mile': 0,\n",
       " '22lb': 0,\n",
       " '22nd': 0,\n",
       " '22th': 0,\n",
       " '235ml': 0,\n",
       " '23rd': 0,\n",
       " '23th': 0,\n",
       " '23yr': 0,\n",
       " '24h': 0,\n",
       " '24th': 0,\n",
       " '24w': 0,\n",
       " '24x7': 0,\n",
       " '24year': 0,\n",
       " '256mb': 0,\n",
       " '25k': 0,\n",
       " '25ml': 0,\n",
       " '25pm': 0,\n",
       " '25px': 0,\n",
       " '25th': 0,\n",
       " '25year': 0,\n",
       " '26th': 0,\n",
       " '275lb': 0,\n",
       " '27b': 0,\n",
       " '27th': 0,\n",
       " '280x720': 0,\n",
       " '28am': 0,\n",
       " '28th': 0,\n",
       " '29june': 0,\n",
       " '29nd': 0,\n",
       " '29th': 0,\n",
       " '2am': 0,\n",
       " '2b': 0,\n",
       " '2d': 0,\n",
       " '2everi': 0,\n",
       " '2f10': 0,\n",
       " '2fjournal': 0,\n",
       " '2ft': 0,\n",
       " '2g': 0,\n",
       " '2gb': 0,\n",
       " '2gether': 0,\n",
       " '2gthr': 0,\n",
       " '2ish': 0,\n",
       " '2k': 0,\n",
       " '2l': 0,\n",
       " '2lb': 0,\n",
       " '2m': 0,\n",
       " '2nd': 0,\n",
       " '2pm': 0,\n",
       " '2px': 0,\n",
       " '2u': 0,\n",
       " '2x': 0,\n",
       " '2x2': 0,\n",
       " '2ºc': 0,\n",
       " '2πr': 0,\n",
       " '300dpi': 0,\n",
       " '300for': 0,\n",
       " '300g': 0,\n",
       " '300mile': 0,\n",
       " '300rpm': 0,\n",
       " '302a': 0,\n",
       " '30am': 0,\n",
       " '30d': 0,\n",
       " '30gm': 0,\n",
       " '30kg': 0,\n",
       " '30mg': 0,\n",
       " '30pm': 0,\n",
       " '30rockrecapgangwayforfoot': 0,\n",
       " '30th': 0,\n",
       " '31st': 0,\n",
       " '32gb': 0,\n",
       " '32mb': 0,\n",
       " '32pm': 0,\n",
       " '33ghz': 0,\n",
       " '33yr': 0,\n",
       " '340gm': 0,\n",
       " '345º': 0,\n",
       " '348º': 0,\n",
       " '34th': 0,\n",
       " '35000rupiah': 0,\n",
       " '350º': 0,\n",
       " '350ºf': 0,\n",
       " '35k': 0,\n",
       " '35mm': 0,\n",
       " '362ºc': 0,\n",
       " '365day': 0,\n",
       " '366x768': 0,\n",
       " '36m': 0,\n",
       " '36million': 0,\n",
       " '36th': 0,\n",
       " '37signal': 0,\n",
       " '380w': 0,\n",
       " '389pp': 0,\n",
       " '39a': 0,\n",
       " '39fork': 0,\n",
       " '39rm': 0,\n",
       " '39th': 0,\n",
       " '3a': 0,\n",
       " '3am': 0,\n",
       " '3b': 0,\n",
       " '3bn': 0,\n",
       " '3bt': 0,\n",
       " '3d': 0,\n",
       " '3dstripbook': 0,\n",
       " '3dtv': 0,\n",
       " '3e': 0,\n",
       " '3g': 0,\n",
       " '3k': 0,\n",
       " '3lb': 0,\n",
       " '3m': 0,\n",
       " '3mi': 0,\n",
       " '3mm154m2ndi': 0,\n",
       " '3oth': 0,\n",
       " '3pm': 0,\n",
       " '3px': 0,\n",
       " '3rd': 0,\n",
       " '3x': 0,\n",
       " '3x3': 0,\n",
       " '40': 0,\n",
       " '400k': 0,\n",
       " '400m': 0,\n",
       " '400º': 0,\n",
       " '401k': 0,\n",
       " '4050r': 0,\n",
       " '40c': 0,\n",
       " '40gb': 0,\n",
       " '40k': 0,\n",
       " '40lb': 0,\n",
       " '40mg': 0,\n",
       " '40pm': 0,\n",
       " '40th': 0,\n",
       " '42nd': 0,\n",
       " '42s02': 0,\n",
       " '436a': 0,\n",
       " '436awa': 0,\n",
       " '450w': 0,\n",
       " '45pm': 0,\n",
       " '460w': 0,\n",
       " '460xvr': 0,\n",
       " '465m': 0,\n",
       " '46bn': 0,\n",
       " '470ml': 0,\n",
       " '474ml': 0,\n",
       " '47gb': 0,\n",
       " '480x360': 0,\n",
       " '48gb': 0,\n",
       " '4ad': 0,\n",
       " '4am': 0,\n",
       " '4chan': 0,\n",
       " '4cm': 0,\n",
       " '4everfrom': 0,\n",
       " '4evr': 0,\n",
       " '4f': 0,\n",
       " '4g': 0,\n",
       " '4gb': 0,\n",
       " '4get': 0,\n",
       " '4gt': 0,\n",
       " '4h': 0,\n",
       " '4h30': 0,\n",
       " '4k': 0,\n",
       " '4lb': 0,\n",
       " '4m': 0,\n",
       " '4pm': 0,\n",
       " '4pt': 0,\n",
       " '4rm': 0,\n",
       " '4rom': 0,\n",
       " '4sitwc': 0,\n",
       " '4sq': 0,\n",
       " '4squar': 0,\n",
       " '4th': 0,\n",
       " '4trn': 0,\n",
       " '4wd': 0,\n",
       " '4x': 0,\n",
       " '4x4': 0,\n",
       " '4yr': 0,\n",
       " '50': 0,\n",
       " '5001a': 0,\n",
       " '500g': 0,\n",
       " '500gb': 0,\n",
       " '500lb': 0,\n",
       " '500ml': 0,\n",
       " '500with': 0,\n",
       " '50cm': 0,\n",
       " '50e': 0,\n",
       " '50gb': 0,\n",
       " '50gm': 0,\n",
       " '50k': 0,\n",
       " '50km': 0,\n",
       " '50lb': 0,\n",
       " '50mb': 0,\n",
       " '50mg': 0,\n",
       " '50th': 0,\n",
       " '50w': 0,\n",
       " '50x50': 0,\n",
       " '50x60': 0,\n",
       " '50x66': 0,\n",
       " '512k': 0,\n",
       " '512mb': 0,\n",
       " '51st': 0,\n",
       " '5200r': 0,\n",
       " '55gm': 0,\n",
       " '55mm': 0,\n",
       " '55pm': 0,\n",
       " '55th': 0,\n",
       " '573pp': 0,\n",
       " '59p': 0,\n",
       " '59pm': 0,\n",
       " '5am': 0,\n",
       " '5bn': 0,\n",
       " '5ft': 0,\n",
       " '5g': 0,\n",
       " '5gb': 0,\n",
       " '5k': 0,\n",
       " '5km': 0,\n",
       " '5mb': 0,\n",
       " '5mg': 0,\n",
       " '5ml': 0,\n",
       " '5pm': 0,\n",
       " '5th': 0,\n",
       " '5x': 0,\n",
       " '5x7': 0,\n",
       " '5º': 0,\n",
       " '60': 0,\n",
       " '600mg': 0,\n",
       " '60f': 0,\n",
       " '60k': 0,\n",
       " '60ml': 0,\n",
       " '60x60': 0,\n",
       " '62nd': 0,\n",
       " '640x480': 0,\n",
       " '64bit': 0,\n",
       " '64gb': 0,\n",
       " '64º': 0,\n",
       " '65k': 0,\n",
       " '65kg': 0,\n",
       " '65lb': 0,\n",
       " '65m': 0,\n",
       " '65th': 0,\n",
       " '6600r': 0,\n",
       " '66th': 0,\n",
       " '6am': 0,\n",
       " '6b': 0,\n",
       " '6gb': 0,\n",
       " '6hr': 0,\n",
       " '6lb': 0,\n",
       " '6mhz': 0,\n",
       " '6nfqa': 0,\n",
       " '6o': 0,\n",
       " '6pm': 0,\n",
       " '6s': 0,\n",
       " '6th': 0,\n",
       " '6x6': 0,\n",
       " '6yr': 0,\n",
       " '70': 0,\n",
       " '7025jeev': 0,\n",
       " '720p': 0,\n",
       " '72dpi': 0,\n",
       " '72ppi': 0,\n",
       " '730am': 0,\n",
       " '75': 0,\n",
       " '75gm': 0,\n",
       " '75mg': 0,\n",
       " '75th': 0,\n",
       " '77wabc': 0,\n",
       " '785g': 0,\n",
       " '785gmt': 0,\n",
       " '7am': 0,\n",
       " '7e': 0,\n",
       " '7gb': 0,\n",
       " '7kg': 0,\n",
       " '7km': 0,\n",
       " '7lb': 0,\n",
       " '7m': 0,\n",
       " '7m7ezmsthdibhz5bzvynd': 0,\n",
       " '7pm': 0,\n",
       " '7th': 0,\n",
       " '7x7': 0,\n",
       " '80': 0,\n",
       " '800ºc': 0,\n",
       " '80a': 0,\n",
       " '80g': 0,\n",
       " '81mg': 0,\n",
       " '82nd': 0,\n",
       " '83rd': 0,\n",
       " '84th': 0,\n",
       " '87th': 0,\n",
       " '88cm': 0,\n",
       " '89lb': 0,\n",
       " '89th': 0,\n",
       " '8am': 0,\n",
       " '8bn': 0,\n",
       " '8c': 0,\n",
       " '8em': 0,\n",
       " '8gb': 0,\n",
       " '8gm': 0,\n",
       " '8h': 0,\n",
       " '8k': 0,\n",
       " '8kg': 0,\n",
       " '8lb': 0,\n",
       " '8mm': 0,\n",
       " '8pm': 0,\n",
       " '8th': 0,\n",
       " '8x10': 0,\n",
       " '8xbk9q': 0,\n",
       " '90': 0,\n",
       " '900k': 0,\n",
       " '906266p2': 0,\n",
       " '90mg': 0,\n",
       " '90of': 0,\n",
       " '92nd': 0,\n",
       " '94th': 0,\n",
       " '95gm': 0,\n",
       " '969м': 0,\n",
       " '980x': 0,\n",
       " '999th': 0,\n",
       " '9akqia': 0,\n",
       " '9am': 0,\n",
       " '9b': 0,\n",
       " '9c': 0,\n",
       " '9ish': 0,\n",
       " '9lb': 0,\n",
       " '9oz': 0,\n",
       " '9pm': 0,\n",
       " '9pmnj9': 0,\n",
       " '9tbjlg': 0,\n",
       " '9th': 0,\n",
       " '9wiqyv': 0,\n",
       " '9xxrsz': 0,\n",
       " '9yr': 0,\n",
       " 'a2tay9': 0,\n",
       " 'a3': 0,\n",
       " 'a300': 0,\n",
       " 'a4': 0,\n",
       " 'a7x': 0,\n",
       " 'a874': 0,\n",
       " 'aa': 0,\n",
       " 'aaa': 0,\n",
       " 'aaaa': 0,\n",
       " 'aaaaaaaaa': 0,\n",
       " 'aaaaah': 0,\n",
       " 'aaaaand': 0,\n",
       " 'aaaah': 0,\n",
       " 'aaaand': 0,\n",
       " 'aaah': 0,\n",
       " 'aaahhaaa': 0,\n",
       " 'aacross': 0,\n",
       " 'aada': 0,\n",
       " 'aadi': 0,\n",
       " 'aaduvaa': 0,\n",
       " 'aadyqf': 0,\n",
       " 'aafp': 0,\n",
       " 'aah': 0,\n",
       " 'aaj': 0,\n",
       " 'aam': 0,\n",
       " 'aamir': 0,\n",
       " 'aana': 0,\n",
       " 'aandolan': 0,\n",
       " 'aap': 0,\n",
       " 'aarathi': 0,\n",
       " 'aaron': 0,\n",
       " 'aarp': 0,\n",
       " 'aarthi': 0,\n",
       " 'aarush': 0,\n",
       " 'aasai': 0,\n",
       " 'aasif': 0,\n",
       " 'aaspir': 0,\n",
       " 'aata': 0,\n",
       " 'aathu': 0,\n",
       " 'aawvmq': 0,\n",
       " 'aayega': 0,\n",
       " 'ab': 0,\n",
       " 'aba': 0,\n",
       " 'ababa': 0,\n",
       " 'aback': 0,\n",
       " 'abacu': 0,\n",
       " 'abad': 0,\n",
       " 'abaixo': 0,\n",
       " 'abala': 0,\n",
       " 'abandon': 0,\n",
       " 'abandonada': 0,\n",
       " 'abat': 0,\n",
       " 'abb': 0,\n",
       " 'abba': 0,\n",
       " 'abbess': 0,\n",
       " 'abbey': 0,\n",
       " 'abbi': 0,\n",
       " 'abbott': 0,\n",
       " 'abbrevi': 0,\n",
       " 'abc': 0,\n",
       " 'abc800': 0,\n",
       " 'abcd': 0,\n",
       " 'abcedarian': 0,\n",
       " 'abcloc': 0,\n",
       " 'abdalhaleem': 0,\n",
       " 'abdallat': 0,\n",
       " 'abdel': 0,\n",
       " 'abdelwahid': 0,\n",
       " 'abdi': 0,\n",
       " 'abdomen': 0,\n",
       " 'abdomin': 0,\n",
       " 'abdominoplasti': 0,\n",
       " 'abduct': 0,\n",
       " 'abdul': 0,\n",
       " 'abdulmutallab': 0,\n",
       " 'abdulwahid': 0,\n",
       " 'abe': 0,\n",
       " 'abeauti': 0,\n",
       " 'abednego': 0,\n",
       " 'abel': 0,\n",
       " 'abelard': 0,\n",
       " 'abercrombi': 0,\n",
       " 'aberdeen': 0,\n",
       " 'abernathi': 0,\n",
       " 'abet': 0,\n",
       " 'abhik': 0,\n",
       " 'abhinav': 0,\n",
       " 'abhishekontheweb': 0,\n",
       " 'abhor': 0,\n",
       " 'abhyu': 0,\n",
       " 'abhyuday': 0,\n",
       " 'abid': 0,\n",
       " 'abigail': 0,\n",
       " 'abil': 0,\n",
       " 'abilifi': 0,\n",
       " 'abirami': 0,\n",
       " 'abject': 0,\n",
       " 'abl': 0,\n",
       " 'ablaz': 0,\n",
       " 'abli': 0,\n",
       " 'ablum': 0,\n",
       " 'abn': 0,\n",
       " 'abnorm': 0,\n",
       " 'abnorrm': 0,\n",
       " 'aboard': 0,\n",
       " 'abod': 0,\n",
       " 'aboiut': 0,\n",
       " 'aboleth': 0,\n",
       " 'abolish': 0,\n",
       " 'abolit': 0,\n",
       " 'abolitionist': 0,\n",
       " 'abomin': 0,\n",
       " 'aborigin': 0,\n",
       " 'abort': 0,\n",
       " 'abou': 0,\n",
       " 'abound': 0,\n",
       " 'abouyt': 0,\n",
       " 'abra': 0,\n",
       " 'abracadabra': 0,\n",
       " 'abraham': 0,\n",
       " 'abram': 0,\n",
       " 'abranch': 0,\n",
       " 'abras': 0,\n",
       " 'abreast': 0,\n",
       " 'abri': 0,\n",
       " 'abridg': 0,\n",
       " 'abriu': 0,\n",
       " 'abroad': 0,\n",
       " 'abrupt': 0,\n",
       " 'abruptli': 0,\n",
       " 'abscenc': 0,\n",
       " 'abscess': 0,\n",
       " 'abscond': 0,\n",
       " 'abseil': 0,\n",
       " 'absenc': 0,\n",
       " 'absens': 0,\n",
       " 'absent': 0,\n",
       " 'absente': 0,\n",
       " 'absinth': 0,\n",
       " 'absolu': 0,\n",
       " 'absolut': 0,\n",
       " 'absolutli': 0,\n",
       " 'absorb': 0,\n",
       " 'abstain': 0,\n",
       " 'abstent': 0,\n",
       " 'abstin': 0,\n",
       " 'abstract': 0,\n",
       " 'abstractli': 0,\n",
       " 'abstruct': 0,\n",
       " 'absurd': 0,\n",
       " 'absurdli': 0,\n",
       " 'absurdum': 0,\n",
       " 'abt': 0,\n",
       " 'abu': 0,\n",
       " 'abuja': 0,\n",
       " 'abukhalil': 0,\n",
       " 'abulm': 0,\n",
       " 'abund': 0,\n",
       " 'abundantli': 0,\n",
       " 'abus': 0,\n",
       " 'abut': 0,\n",
       " 'abuzz': 0,\n",
       " 'abysm': 0,\n",
       " 'abyss': 0,\n",
       " 'abyssinia': 0,\n",
       " 'ac': 0,\n",
       " 'ac360': 0,\n",
       " 'acaba': 0,\n",
       " 'acabara': 0,\n",
       " 'academ': 0,\n",
       " 'academi': 0,\n",
       " 'academia': 0,\n",
       " 'academo': 0,\n",
       " 'acai': 0,\n",
       " 'acap': 0,\n",
       " 'acapulco': 0,\n",
       " 'acc': 0,\n",
       " 'accel': 0,\n",
       " 'acceler': 0,\n",
       " 'accent': 0,\n",
       " 'accentu': 0,\n",
       " 'accept': 1,\n",
       " 'acceso': 0,\n",
       " 'access': 0,\n",
       " 'accesscontrol': 0,\n",
       " 'accessoir': 0,\n",
       " 'accessor': 0,\n",
       " 'accessori': 0,\n",
       " 'accessoris': 0,\n",
       " 'accid': 0,\n",
       " 'accident': 0,\n",
       " 'acclaim': 0,\n",
       " 'acclim': 0,\n",
       " 'acclimatis': 0,\n",
       " 'accolad': 0,\n",
       " 'accommod': 0,\n",
       " 'accomod': 0,\n",
       " 'accompagn': 0,\n",
       " 'accompani': 0,\n",
       " 'accompic': 0,\n",
       " 'accomplish': 0,\n",
       " 'accoplish': 0,\n",
       " 'accord': 0,\n",
       " 'accordin': 0,\n",
       " 'accordingli': 0,\n",
       " 'account': 0,\n",
       " 'accountdata': 0,\n",
       " 'accoutr': 0,\n",
       " 'accpac': 0,\n",
       " 'accra': 0,\n",
       " 'accret': 0,\n",
       " 'accross': 0,\n",
       " 'accru': 0,\n",
       " 'acctual': 0,\n",
       " 'accumul': 0,\n",
       " 'accur': 0,\n",
       " 'accuraci': 0,\n",
       " 'accus': 0,\n",
       " 'accusatori': 0,\n",
       " 'accustom': 0,\n",
       " 'accuweath': 0,\n",
       " 'ace': 0,\n",
       " 'aceitarei': 0,\n",
       " 'aceito': 0,\n",
       " 'aceo': 0,\n",
       " 'acept': 0,\n",
       " 'acessori': 0,\n",
       " 'acetaminophen': 0,\n",
       " 'acetamophin': 0,\n",
       " 'ach': 0,\n",
       " 'achaar': 0,\n",
       " 'achalasia': 0,\n",
       " 'achalsia': 0,\n",
       " 'acheiv': 0,\n",
       " 'achiev': 0,\n",
       " 'achil': 0,\n",
       " 'achincalhado': 0,\n",
       " 'achingli': 0,\n",
       " 'achu': 0,\n",
       " 'achudhanandan': 0,\n",
       " 'acid': 0,\n",
       " 'acit': 0,\n",
       " 'ack': 0,\n",
       " 'ackatsi': 0,\n",
       " 'acker': 0,\n",
       " 'acknowledg': 0,\n",
       " 'acl': 0,\n",
       " 'aclu': 0,\n",
       " 'acm': 0,\n",
       " 'acn': 0,\n",
       " 'acolyt': 0,\n",
       " 'acompagn': 0,\n",
       " 'acontec': 0,\n",
       " 'acontecem': 0,\n",
       " 'aconteceu': 0,\n",
       " 'acontecimento': 0,\n",
       " 'acorn': 0,\n",
       " 'acoust': 0,\n",
       " 'acp': 0,\n",
       " 'acquaint': 0,\n",
       " 'acquaintanceship': 0,\n",
       " 'acquiesc': 0,\n",
       " 'acquir': 0,\n",
       " 'acquisit': 0,\n",
       " 'acquit': 0,\n",
       " 'acr': 0,\n",
       " 'acredito': 0,\n",
       " 'acrimoni': 0,\n",
       " 'acrooss': 0,\n",
       " 'across': 0,\n",
       " 'acrossth': 0,\n",
       " 'acrost': 0,\n",
       " 'acryl': 0,\n",
       " 'act': 1,\n",
       " 'acta': 0,\n",
       " 'action': 0,\n",
       " 'actionaround': 0,\n",
       " 'actionscript': 0,\n",
       " 'activ': 0,\n",
       " 'activis': 0,\n",
       " 'activist': 0,\n",
       " 'activit': 0,\n",
       " 'activli': 0,\n",
       " 'acton': 0,\n",
       " 'actor': 0,\n",
       " 'actress': 0,\n",
       " 'actressi': 0,\n",
       " 'actsasvers': 0,\n",
       " 'actual': 0,\n",
       " 'actuali': 0,\n",
       " 'actuat': 0,\n",
       " 'acupunctur': 0,\n",
       " 'acura': 0,\n",
       " 'acut': 0,\n",
       " 'acuvu': 0,\n",
       " 'acá': 0,\n",
       " 'ad': 0,\n",
       " 'ada': 0,\n",
       " 'adag': 0,\n",
       " 'adak': 0,\n",
       " 'adakah': 0,\n",
       " 'adakamaana': 0,\n",
       " 'adala': 0,\n",
       " 'adalah': 0,\n",
       " 'adam': 0,\n",
       " 'adamantli': 0,\n",
       " 'adambakkam': 0,\n",
       " 'adap': 0,\n",
       " 'adapt': 2,\n",
       " 'adaverallam': 0,\n",
       " 'aday': 0,\n",
       " 'adblock': 0,\n",
       " 'add': 0,\n",
       " 'addam': 0,\n",
       " 'addenbrook': 0,\n",
       " 'addendum': 0,\n",
       " 'adderal': 0,\n",
       " 'addi': 0,\n",
       " 'addict': 0,\n",
       " 'addit': 0,\n",
       " 'additon': 0,\n",
       " 'additud': 0,\n",
       " 'addl': 0,\n",
       " 'addon': 0,\n",
       " 'address': 0,\n",
       " 'addressbar': 0,\n",
       " 'addresse': 0,\n",
       " 'addtion': 0,\n",
       " 'addustour': 0,\n",
       " 'addybel': 0,\n",
       " 'addycat': 0,\n",
       " 'addyson': 0,\n",
       " 'ade': 0,\n",
       " 'adebayor': 0,\n",
       " 'adej': 0,\n",
       " 'adel': 0,\n",
       " 'adela': 0,\n",
       " 'adelaid': 0,\n",
       " 'adelaidian': 0,\n",
       " 'adelson': 0,\n",
       " 'adelstein': 0,\n",
       " 'aden': 0,\n",
       " 'adenoid': 0,\n",
       " 'adenoma': 0,\n",
       " 'adenovirida': 0,\n",
       " 'adenoviru': 0,\n",
       " 'adept': 0,\n",
       " 'adequ': 0,\n",
       " 'adf': 0,\n",
       " 'adgroup': 0,\n",
       " 'adha': 0,\n",
       " 'adhd': 0,\n",
       " 'adhdan': 0,\n",
       " 'adher': 0,\n",
       " 'adhes': 0,\n",
       " 'adhirade': 0,\n",
       " 'adhu': 0,\n",
       " 'adhukellam': 0,\n",
       " 'adhunik': 0,\n",
       " 'adhérent': 0,\n",
       " 'adi': 0,\n",
       " 'adicha': 0,\n",
       " 'adichaan': 0,\n",
       " 'adida': 0,\n",
       " 'adieu': 0,\n",
       " 'adikara': 0,\n",
       " 'adil': 0,\n",
       " 'adina': 0,\n",
       " 'adio': 0,\n",
       " 'adippa': 0,\n",
       " 'adirondack': 0,\n",
       " 'adithya': 0,\n",
       " 'aditud': 0,\n",
       " 'adjac': 0,\n",
       " 'adjcent': 0,\n",
       " 'adject': 0,\n",
       " 'adjoin': 0,\n",
       " 'adjourn': 0,\n",
       " 'adjud': 0,\n",
       " 'adjust': 0,\n",
       " 'admi': 0,\n",
       " 'admin': 0,\n",
       " 'administ': 0,\n",
       " 'administr': 0,\n",
       " 'admir': 0,\n",
       " 'admiss': 0,\n",
       " 'admist': 0,\n",
       " 'admit': 0,\n",
       " 'admitt': 0,\n",
       " 'admittedli': 0,\n",
       " 'admob': 0,\n",
       " 'admonit': 0,\n",
       " 'admor': 0,\n",
       " 'adn': 0,\n",
       " 'ado': 0,\n",
       " 'adob': 0,\n",
       " 'adobo': 0,\n",
       " 'adolesc': 0,\n",
       " 'adolf': 0,\n",
       " 'adolphu': 0,\n",
       " 'adopt': 0,\n",
       " 'ador': 0,\n",
       " 'adora': 0,\n",
       " 'adoraal': 0,\n",
       " 'adorablest': 0,\n",
       " 'adorin': 0,\n",
       " 'adorn': 0,\n",
       " 'adoré': 0,\n",
       " 'adoxograph': 0,\n",
       " 'adra': 0,\n",
       " 'adren': 0,\n",
       " 'adrenalin': 0,\n",
       " 'adrian': 0,\n",
       " 'adriana': 0,\n",
       " 'adriat': 0,\n",
       " 'adrienn': 0,\n",
       " 'adrift': 0,\n",
       " 'adroitli': 0,\n",
       " 'adsens': 0,\n",
       " 'adsl': 0,\n",
       " 'adt': 0,\n",
       " 'adu': 0,\n",
       " 'aducharm': 0,\n",
       " 'aduchifyin': 0,\n",
       " 'adult': 0,\n",
       " 'adultaraisin': 0,\n",
       " 'adulteri': 0,\n",
       " 'adulthood': 0,\n",
       " 'adurai': 0,\n",
       " 'advair': 0,\n",
       " 'advanc': 0,\n",
       " 'advani': 0,\n",
       " 'advantag': 0,\n",
       " 'advent': 0,\n",
       " 'adventur': 0,\n",
       " 'adventureland': 0,\n",
       " 'adverb': 0,\n",
       " 'advers': 0,\n",
       " 'adversari': 0,\n",
       " 'advert': 0,\n",
       " 'advertis': 0,\n",
       " 'advic': 0,\n",
       " 'advis': 0,\n",
       " 'advise': 0,\n",
       " 'advisor': 0,\n",
       " 'advisori': 0,\n",
       " 'advoc': 0,\n",
       " 'advocaci': 0,\n",
       " 'adwar': 0,\n",
       " 'adword': 0,\n",
       " 'adyar': 0,\n",
       " 'ae': 0,\n",
       " 'aea': 0,\n",
       " 'aer9': 0,\n",
       " 'aerat': 0,\n",
       " 'aeri': 0,\n",
       " 'aerial': 0,\n",
       " 'aero': 0,\n",
       " 'aerob': 0,\n",
       " 'aeroballoon': 0,\n",
       " 'aerodynam': 0,\n",
       " 'aeroplan': 0,\n",
       " 'aerosmith': 0,\n",
       " 'aeschylu': 0,\n",
       " 'aesthet': 0,\n",
       " 'aesthtic': 0,\n",
       " 'aether': 0,\n",
       " 'aethest': 0,\n",
       " 'afa': 0,\n",
       " 'afaik': 0,\n",
       " 'afar': 0,\n",
       " 'afarensi': 0,\n",
       " 'afb': 0,\n",
       " 'afew': 0,\n",
       " 'affabl': 0,\n",
       " 'affair': 0,\n",
       " 'affect': 0,\n",
       " 'affection': 0,\n",
       " 'affem': 0,\n",
       " 'affettati': 0,\n",
       " 'affidavit': 0,\n",
       " 'affili': 0,\n",
       " 'affin': 0,\n",
       " 'affirm': 0,\n",
       " 'affix': 0,\n",
       " 'affleck': 0,\n",
       " 'afflict': 0,\n",
       " 'affluent': 0,\n",
       " 'afford': 0,\n",
       " 'affort': 0,\n",
       " 'affront': 0,\n",
       " 'afghan': 0,\n",
       " 'afghani': 0,\n",
       " 'afghanistan': 0,\n",
       " 'aficionado': 0,\n",
       " 'afield': 0,\n",
       " 'afikomen': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_per_doc_completo[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo mismo pero con el vocabulario reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_red_words = dict(sorted(reduced_words.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocabulario reducido ##\n",
    "# blog_df\n",
    "\n",
    "freq_per_doc_reducido = dict()\n",
    "for d in doc_words_per_doc.keys():\n",
    "    freq_per_doc_reducido[d] = dict().fromkeys(order_red_words.keys())\n",
    "    for t in order_red_words.keys():\n",
    "        freq_per_doc_reducido[d][t] = countOf(doc_words_per_doc[d], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_per_doc_reducido[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el modelado en el esquema tf. Procederemos a guardarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resultados\\\\tfidf\\\\tf_completo.json', 'w') as f:\n",
    "    f.write(json.dumps(freq_per_doc_completo))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resultados\\\\tfidf\\\\tf_reducido.json', 'w') as f:\n",
    "    f.write(json.dumps(freq_per_doc_reducido))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos los diccionarios de frecuencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "with open('resultados\\\\tfidf\\\\tf_completo.json', 'r') as file:\n",
    "    freq_per_doc_completo = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resultados\\\\tfidf\\\\tf_reducido.json', 'r') as file:\n",
    "    freq_per_doc_reducido = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, la representación de los documentos según el esquema tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero, calculamos N_t (# de docs con el término)\n",
    "from math import log\n",
    "\n",
    "#order_red_words\n",
    "N_t = dict().fromkeys(order_og_words.keys())\n",
    "N_t = { x : 0 for x in N_t}\n",
    "for i in N_t.keys():\n",
    "    for j in freq_per_doc_completo.keys():\n",
    "        if freq_per_doc_completo[j][i] != 0:\n",
    "            N_t[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando idf_i\n",
    "doc_count = len(blog_df)\n",
    "idf_i = dict().fromkeys(N_t.keys())\n",
    "for i in idf_i.keys():\n",
    "    idf_i[i] = 0\n",
    "    if N_t[i]:\n",
    "        idf_i[i] = log(doc_count / N_t[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_i['chile']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order_og_words_idf = dict().fromkeys(order_og_words.keys())\n",
    "#for i in order_og_words.keys():\n",
    "#    order_og_words_idf[i] = order_og_words[i] * idf_i[i]\n",
    "    \n",
    "freq_per_doc_idf = dict()\n",
    "for i in freq_per_doc_completo.keys():\n",
    "    freq_per_doc_idf[i] = dict().fromkeys(order_og_words.keys())\n",
    "    for j in order_og_words.keys():\n",
    "        freq_per_doc_idf[i][j] = freq_per_doc_completo[i][j] * idf_i[j]\n",
    "        \n",
    "# freq_per_doc_2   -> vector d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.38770923908104"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_per_doc_idf[1]['altough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* con vocabulario reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order_red_words_idf = dict().fromkeys(order_red_words.keys())\n",
    "#for i in order_red_words.keys():\n",
    "#    order_red_words_idf[i] = order_red_words[i] * idf_i[i]\n",
    "    \n",
    "freq_per_doc_idf_red = dict()\n",
    "for i in freq_per_doc_reducido.keys():\n",
    "    freq_per_doc_idf_red[i] = dict().fromkeys(order_red_words.keys())\n",
    "    for j in order_red_words.keys():\n",
    "        freq_per_doc_idf_red[i][j] = freq_per_doc_reducido[i][j] * idf_i[j]\n",
    "        \n",
    "# dict_freq_2         -> vector q\n",
    "# freq_per_doc_2   -> vector d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.38770923908104"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_per_doc_idf_red[1]['altough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la representación de los documentos con tf-idf. Se procederá a guardar los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resultados\\\\tfidf\\\\idf_completo.json', 'w') as f:\n",
    "    f.write(json.dumps(freq_per_doc_idf))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resultados\\\\tfidf\\\\idf_reducido.json', 'w') as f:\n",
    "    f.write(json.dumps(freq_per_doc_idf_red))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bac3ed83652cd3fb2be2ab4720f126b858190479445937cf7f7241afc53dfb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
